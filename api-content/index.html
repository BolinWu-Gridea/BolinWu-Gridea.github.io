{"posts":[{"title":"test","content":"test ","link":"https://BolinWu-Gridea.github.io/post/test/"},{"title":"Classification of Baseball Player Technical Statistics by Decision Tree and Random Forest","content":"Introduction Tree-based methods are conceptually easy to comprehend and they render advantages like easy visualization and data-preprocessing. It is a powerful tool for classification. In this post I will introduce how to classify baseball player technical statistics by Decision Tree and Random Forest from algorithm coding to package usage. Content: Showing the algorithm of Decision Tree by R, which including tree splitting, tree grwoing, bagging, prediction, etc. Runing Random Forest and XGBoost with the help of randomForest package Reference: The Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie et al. Mans Magnusson (2020). uuml: R Content for the Introduction to Machine Learning. Course at Uppsala University. R package version 0.2.0. Decision Tree Data Pre-processing First, we need to lead the dataset Hitters provided in the refereced uuml package. This dataset consists of the salary of many baseball players and their relevant technical statistics. Here we assume that we only care about three columns:&quot;Years&quot;, &quot;Hits&quot; and &quot;Salary&quot;. The missing values are imputed by listwise deletion. And we set the first 30 ovservations as test set and the rest as training set. # install relevant packages # remotes::install_github(&quot;MansMeg/IntroML&quot;,subdir = &quot;rpackage&quot;) # install.packages(&quot;randomForest&quot;) # install.packages(&quot;xgboost&quot;) # loead the packages library(xgboost) library(randomForest) # load the data library(uuml) data(&quot;Hitters&quot;) # In the task we only care about three columns:&quot;Years&quot;, &quot;Hits&quot; and &quot;Salary&quot; # and we need to excluede the NA values # So we need to pre-process the data Hitters = Hitters[,c(&quot;Years&quot;, &quot;Hits&quot;,&quot;Salary&quot;)] # get rid of NA Hitters &lt;- Hitters[complete.cases(Hitters),] # set aside test set and training set X_test &lt;- Hitters[1:30, c(&quot;Years&quot;, &quot;Hits&quot;)] y_test &lt;- Hitters[1:30, c(&quot;Salary&quot;)] X_train &lt;- Hitters[31:nrow(Hitters), c(&quot;Years&quot;, &quot;Hits&quot;)] y_train &lt;- Hitters[31:nrow(Hitters), c(&quot;Salary&quot;)] Tree Splitting Now let us see how to do the first split. Please note that here does not involve tree growing. Concept The alforithm we will use is from the referenced book The Elements of Statistical Learning: Data Mining, Inference, and Prediction (ESL), P307. We are seeking to splitting variable j and split point s that meet: minj,s[minc1∑xi∈R1(j,s)(yi−c1)2+minc2∑xi∈R2(j,s)(yi−c2)2]\\begin{aligned} min_{j,s} [min_{c1} \\sum_{x_{i}\\in R_{1}(j,s)}(y_{i} - c_{1})^{2} + min_{c_{2}} \\sum_{x_{i} \\in R_{2}(j,s)}(y_{i} - c_{2})^{2} ] \\end{aligned} minj,s​[minc1​xi​∈R1​(j,s)∑​(yi​−c1​)2+minc2​​xi​∈R2​(j,s)∑​(yi​−c2​)2]​ Where the inner minimization with regard to j and s is solved by : c1^=ave(yi∣xi∈R1(j,s))c2^=ave(yi∣xi∈R2(j,s))\\begin{aligned} \\hat{c_{1}} &amp;= ave (y_{i}| x_{i} \\in R_{1} (j,s)) \\\\ \\hat{c_{2}} &amp;= ave (y_{i}| x_{i} \\in R_{2} (j,s)) \\end{aligned} c1​^​c2​^​​=ave(yi​∣xi​∈R1​(j,s))=ave(yi​∣xi​∈R2​(j,s))​ By first glance, you may get confused by what do those equations mean. Let us use a part of the data to make an illustration: # use a size of 20 X_check &lt;- Hitters[31:50, c(&quot;Years&quot;, &quot;Hits&quot;)] y_check &lt;- Hitters[31:50, c(&quot;Salary&quot;)] head(X_check) ## Years Hits ## -Bob Melvin 2 60 ## -BillyJo Robidoux 2 41 ## -Bill Schroeder 4 46 ## -Chris Bando 6 68 ## -Chris Brown 3 132 ## -Carmen Castillo 5 57 Essentially, what they do can be explained by the following three steps: Let j grind over all the variables of the dataset, which in our case is 2 variables Years and Hits. Let s grind over all the values of jth variable. For example, given the sample data above when j = 1 (Years), s will grind from year = 2 (Bob Melvin) to the year of last player. Allocate each observation according to the given j and s into two groups. And then calculate the mean value of each group, $$\\hat{c_{1}}$$ and $$\\hat{c_{2}}$$. Get the within group scatters by using the function in the min() of the first equation above. Return the j and s that give the smallest within group scatter. This process can be also called greedy method, because we are grinding all the possible values and return the most ideal result. Code To illustrate with R code, we will implement a function that takes data set X, the label y, and a minimal leaf size l. This function will give four outputs: The region that each observation belongs to (R1 and R2), splitting variable j and splitting point s. tree_split = function(X,y,l){ # store the split point S = matrix(NA,nrow = nrow(X), ncol = ncol(X)) # store the sum of square SS = matrix(NA,nrow = nrow(X), ncol = ncol(X)) for (j in 1:ncol(X)) { for (k in 1:nrow(X)) { # use the data point as split point s = X[k,j] # get the size in each leaf R1_size = length( which(X[,j] &lt; s) ) R2_size = length( which(X[,j] &gt;= s) ) # proceed if the size of leaf is bigger than the minimum l if (R1_size &gt;= l &amp; R2_size &gt;=l) { # 2.1.3 c1 = mean( y[which(X[,j] &lt; s)] ) # 2.1.4 c2 = mean( y[which(X[,j] &gt;= s)] ) # 2.1.5 SS[k,j] = sum( (y[which(X[,j] &lt; s)] - c1)^2 ) + sum((y[which(X[,j] &gt;= s)] - c2)^2) } else{ # if the leaf is smaller than the minimum, then set to inf SS[k,j] = Inf } S[k,j] = s } } # find the index of Matix with smallest value j = which(SS == min(SS), arr.ind = TRUE)[1,2]; s = X[which(SS == min(SS), arr.ind = TRUE)[1,1],j]; R1 = which(X[,j] &lt; s); R2 = which(X[,j] &gt;= s) return(list(j = j, s = s, R1 = R1, R2 = R2 , SS = min(SS) ) ) } Then check with the sample data, assuming the minimal leaf size to be 5: tree_split(X_check, y_check, l = 5) $j col 1 $s [1] 6 $R1 [1] 1 2 3 5 6 9 13 16 18 19 $R2 [1] 4 7 8 10 11 12 14 15 17 20 $SS [1] 1346633 The results seem to be reasonable. What about the first split for the whole training data? tree_split(X_train , y_train , l = 5 ) $j col 1 $s [1] 5 $R1 [1] 1 2 3 5 9 13 16 18 19 21 22 [12] 28 30 36 38 40 41 42 45 51 55 57 [23] 73 75 78 79 80 88 89 94 96 99 100 [34] 102 104 107 113 114 118 119 121 128 130 133 [45] 134 138 139 141 142 143 145 146 147 149 151 [56] 152 155 157 167 178 180 183 184 185 187 191 [67] 192 193 194 197 198 199 204 206 210 211 216 [78] 220 222 227 228 $R2 [1] 4 6 7 8 10 11 12 14 15 17 20 [12] 23 24 25 26 27 29 31 32 33 34 35 [23] 37 39 43 44 46 47 48 49 50 52 53 [34] 54 56 58 59 60 61 62 63 64 65 66 [45] 67 68 69 70 71 72 74 76 77 81 82 [56] 83 84 85 86 87 90 91 92 93 95 97 [67] 98 101 103 105 106 108 109 110 111 112 115 [78] 116 117 120 122 123 124 125 126 127 129 131 [89] 132 135 136 137 140 144 148 150 153 154 156 [100] 158 159 160 161 162 163 164 165 166 168 169 [111] 170 171 172 173 174 175 176 177 179 181 182 [122] 186 188 189 190 195 196 200 201 202 203 205 [133] 207 208 209 212 213 214 215 217 218 219 221 [144] 223 224 225 226 229 230 231 232 233 $SS [1] 38464163 The fist split variate is j =1, which is year. The value is 5. If year is smaller than 5, then the observations go to R1, otherwise go to R2. Tree Growing In this part, I will first show the code and then illustrate it. Code Conceptually, tree growing is easy to understand: we looping pre-defined tree splitting until the generated leaves are so small that can not be further splitted (leaf size &lt; 2l). However, it is a bit difficult to implement tree growing by coding. Here I will make a function that takes same X,y, and l. It returns a data frame including j, s, gamma, R1_i and R2_i. Gamma is a metric of within group scatter. The R1_i and R2_i indicates which row of data frame to go next. max_num_leaf = 7 #this does not affact the output, just set the max depth of the tree, the redundant part will show NA grow_tree = function(X,y,l){ # make the matrix to store the data S = matrix(NA,nrow = max_num_leaf, ncol = nrow(X)) j = matrix(NA,nrow = max_num_leaf, ncol = 1) s = matrix(NA,nrow = max_num_leaf, ncol = 1) gamma_m = matrix(NA,nrow = max_num_leaf, ncol = 1) R1_i = rep(NA, length = max_num_leaf); R2_i = rep(NA, length = max_num_leaf) # the initial value S[1,] = c(1 : nrow(X)) M = 1 m = 1 while (m &lt;= M ) { # loop until the size is too small to be splitted if ( length(S[m,][!is.na(S[m,])])&gt;= (2*l) ){ # given a specific m: # get leaf size after split len_col = length((tree_split(X[S[m,],], y[S[m,]], l = l)$R1)) S[M+1,1:len_col ] = tree_split(X[S[m,],], y[S[m,]], l = l)$R1 len_col = length((tree_split(X[S[m,],], y[S[m,]], l = l)$R2)) S[M+2,1:len_col] = tree_split(X[S[m,],], y[S[m,]], l = l)$R2 # get the split variable and point j[m] = tree_split(X[S[m,],], y[S[m,]], l = l)$j s[m] = tree_split(X[S[m,],], y[S[m,]], l = l)$s # move on R1_i[m] = M+1 ; R2_i[m] = M + 2 M = M +2 } else { # when the size is too small, just return gamma, stop increasing M gamma_m [m]= mean( y[S[m,]], na.rm = T ) } m = m + 1 } return(data.frame( j = j, s = s , R1_i=R1_i, R2_i = R2_i, gamma = gamma_m )) } Explanation Here my code may seem a bit too much. I believe that different people will have different approaches to build the algorithm and if you who are reading this post have a neater way please let me know, thank you 😃 Instead of explaining the my code line by line, I would like to explain the general concept with the help of my scrach. Sorry if it is a bit ugly. An ugly scratch The key is to use m (green) and M (yellow). The m denotes the index of splitted leaves, the M denotes the total number of leaves given an iteration. Therefore, as long as when the size of leaf m is bigger than 2l, m increases by 1 step while M goes by 2 steps per iteration (since there are two new leaves after each split). It is kind of like m is chasing M, and M stops when leaves stop growing and m stops when it catches M. Every time when m goes one step, it activates tree splitting function and gathers useful infomation. Let us try out with the sample data: tr = grow_tree(X_check,y_check,l = 5) tr ## j s R1_i R2_i gamma ## 1 6 2 3 NA ## 1 4 4 5 NA ## 2 102 6 7 NA ## NA NA NA NA 317 ## NA NA NA NA 496 ## NA NA NA NA 274 ## NA NA NA NA 539 This data frame can be regarded as a &quot;map&quot;. For example, for observation Bob Melvin; year = 2 hits = 60, firstly since year&lt;6, it follows R2_i = 3, going to 3rd row of the data frame. Secondly, since hits&lt;102, it follows R2_i = 7, going to the 7th row. Thirdly, since there are only NA for indicating next step, the 7th row is its destination. Prediction Finally, we are at an exciting part, predicing a new observation given our pre-trained decision tree! The basic idea of prediction is following the output dataframe of the tr. As is mentioned above, R1_i and R2_i indicates the row of the dataframe to go next like a map. It is stopped until the row shows up NA for the first 4 columns. This function is to predict the classification gamma of new observations. predict_with_tree = function(newdata, tree){ pred = matrix(NA,nrow =1 , ncol = ncol(newdata)) for (i in 1: nrow(newdata)) { # start with m =1, the first row m =1 ; s = tree[m,2] ; j = tree[m,1] while (!is.na(tree[m,1])) { if (newdata[i,j] &lt;s) { # follow R1_i m = tree[m,3];s = tree[m,2] ; j = tree[m,1] }else{# follow R2_i m = tree[m,4];s = tree[m,2] ; j = tree[m,1] } pred[i] = tree[m,5] } } return(pred) } Let us pre: X_new &lt;- Hitters[51:52, c(&quot;Years&quot;, &quot;Hits&quot;)] y_new &lt;- Hitters[51:52, c(&quot;Salary&quot;)] predict_with_tree(newdata = X_new, tree = tr) ## [,1] [,2] ## [1,] 317 496 The gamma of first observation is 317 and the second is 396. What is the mean square error on the test set for a tree trained on the whole training data? # set a large maximum tree depth max_num_leaf = 50 # since we have more observations than the check data # set the minimum leaf size = 10 tr = grow_tree(X_train,y_train,l = 10) pred_tr = predict_with_tree(newdata = X_test, tree = tr) MSE = mean((pred_tr - y_test) ^2 ) cat(&quot;MSE =&quot;,MSE) ## MSE = 78395.21 Bagging Concept The basic idea of bagged tree regression is that we draw with replacement a random sample of N units from the original sample and fit a prediction, then we repeat it B times. In the end we weigh together the B predictions and derive the final prediction. The picture on P285, ESL tells us that as the number of Bootstrap samples goes greater, the test error goes smaller then it tends to be a constant which is smaller than the original tree. Figure from P285, ESL Code bagged_tree = function(Xtrain, Ytrain, l, B,Xtest){ sizeN = nrow(Xtrain) # store the predictions bagged_pred = matrix(NA, nrow = B, ncol = nrow(Xtest)) for (i in (1:B)) { # bootstrap sample random_draw = sample( c(1:sizeN ), size = sizeN ,replace = T ) bagged_tr = grow_tree(Xtrain[random_draw,],Ytrain[random_draw],l) bagged_pred_tr = predict_with_tree(newdata = Xtest, tree = bagged_tr) bagged_pred[i,] = bagged_pred_tr i = i + 1 } # the final prediction is the mean of the B trees prediction return(colMeans(bagged_pred) ) } Let us see if the B goes bigger, will RMSE goes smaller: set.seed(100) cat(&quot;B = 10, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=10,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot;, &quot;B = 20, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=20,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot;, &quot;B = 30, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=30,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot; ) ## B = 10, bagged tree RMSE = 312.8866 ## B = 20, bagged tree RMSE = 309.4088 ## B = 30, bagged tree RMSE = 295.2994 The RMSE goes smaller indeed. Random Forest and Boosting Concept The idea of random forest is very similar to bagged tree model. There is only one difference that in bagged tree model, all the features in the bootstrap samples are used. But in the random forest only random subset (without replacement) of features are chosen. The random forest is supposed to give a better performance if the trees are highly correlated. The intuition of boosting is that the training of latter tree is learning from the misclassification of the previous tree. So that the next trained tree is better than the previous tree. For this part, we just need to fit the data into randomForest() function. ntree controls the number of bootstrap samples. To make it comparable, I also set ntree to be 10 which is the same as the previous bagged tree regression. Code train_df = Hitters[31:nrow(Hitters),] rf_mod = randomForest(Salary~ . , data = train_df, ntree = 10) rf_mod ## ## Call: ## randomForest(formula = Salary ~ ., data = train_df, ntree = 10) ## Type of random forest: regression ## Number of trees: 10 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 182725.5 ## % Var explained: 15.89 The variable that are used is only 1. I suppose the reason is that there are only 2 variables in the X train data. According to the rule of thumb, the number of splitted variable is K/3 for regression model. In our case it is 2/3, which is rounded to be 1. Now we can feed the randomForest function with xtest and ytest so that we can get the MSE of the test set. set.seed(100) rf_mod = randomForest(Salary~ . , data = train_df,xtest = X_test ,ytest = y_test, ntree = 10) rf_mod ## Call: ## randomForest(formula = Salary ~ ., data = train_df, xtest = X_test, ytest = y_test, ntree = 10) ## Type of random forest: regression ## Number of trees: 10 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 193201.2 ## % Var explained: 11.06 ## Test set MSE: 67380.79 ## % Var explained: 24.66 cat(&quot;random forest RMSE of test set =&quot;, sqrt(67380.79)) ## random forest RMSE of test set = 259.5781 After reading the XG boosting documentation, I assume the parameter nrounds control the number of the tree therefore I set it to be 10 to make it comparable with the previous results. xgb = xgboost(data = data.matrix(X_train), label = y_train, max.depth =5, eta = 1, nthread = 2, nrounds =10 ) And the RMSE of the predictions can be calculated as follows: y_pred &lt;- predict(xgb, data.matrix(X_test)) cat(&quot;RMSE of xgboost = &quot;, sqrt(mean((y_pred - y_test)^2)) ) ## RMSE of xgboost = 285.6745 The RMSE is bigger than the random forest model. It could be the reason that the sample size is not big enough or I did the tune the prameters in the function well. However, it is better than the bagged tree model. Ending Challenges Conceptually, tree based methods are not difficult to understand. However, depending on your background, it might be difficult to implement them by plain coding. For example when I was coding the tree growing algorithm, I was struggled with grasping m and M. And also it is easy to code the tree growing process when depth = 2 or 3 but it could be hard to generalize it. It requires a good understanding of looping. Nevertheless, the struggling process does help me to understand the algorithm better. I would encourage the reader to get your hand dirty by starting from scratch despite the fact that there are packages which can make it work easily. Tips The graphic illustration of how tree-based methods partitioning feature into a set of rectangles is pretty good. Please check out on ESL P306. ","link":"https://BolinWu-Gridea.github.io/post/2020-02-18-DecisionTree/"},{"title":"The Application of EM Algorithm","content":"Introduction Expectation-maximization (EM) algorithm is a powerful unsupervised machine learning tool. Conceptually, It is quite similar to k-means algorithm, which I shared in this post. However, instead of clustering through estimated means, it cluster through estimating the distributions parameters and then evaluate how likely is each observation belong to distributions. Another difference is that EM uses soft assignment while k-means uses hard assignment. The content includes: The procedure of EM algorithm in the two-component mixture model context. How to apply different parts of the algorithm step-by-step by simulation data. Test the algorithm by using data of Hastie et al. (2009), as well as the built-in faithful and iris dataset. Algorithm evaluation. Prerequisite to read the following blog: Basic knowledge of mixture model, multivariate normal distribution, maximum likelihood. R programming. Main reference: The Elements of Statistical Learning by Hastie et al. (2009) Chapter 8. Mans Magnusson (2020). uuml: R Content for the Introduction to Machine Learning. Course at Uppsala University. R package version 0.2.0. Loading R packages and data Loading the packages and the data that are used in this post. # add this line for installing the uuml package # remotes::install_github(&quot;MansMeg/IntroML&quot;, # subdir = &quot;rpackage&quot;) # load the package and data library(ggplot2) library(uuml) data(&quot;mixture_data&quot;) y = mixture_data data(&quot;iris&quot;) data(&quot;faithful&quot;) Algorithm Procedure In the context of simple two-component mixture model, we would like to model the density of given data by two normal distributions: Y1∼N(μ1,σ12)Y2∼N(μ2,σ22)Y=(1−Δ)Y1+ΔY2\\begin{aligned} Y_{1} &amp; \\sim \\mathrm{N}(\\mu_{1},\\sigma_{1}^{2}) \\\\ Y_{2} &amp; \\sim \\mathrm{N}(\\mu_{2},\\sigma_{2}^{2}) \\\\ Y &amp;= (1 - \\Delta)Y_{1} + \\Delta Y_{2}\\\\ \\end{aligned} Y1​Y2​Y​∼N(μ1​,σ12​)∼N(μ2​,σ22​)=(1−Δ)Y1​+ΔY2​​ Where $$\\Delta \\in (0,1)$$ with $$Pr (\\Delta =1) = \\pi$$. Then the density of Y is gY(y)=(1−π)ϕθ1(y)+πϕθ2(y)\\begin{aligned} g_{Y}(y) = (1 - \\pi) \\phi_{\\theta_{1}}(y) + \\pi \\phi_{\\theta_{2}}(y) \\end{aligned} gY​(y)=(1−π)ϕθ1​​(y)+πϕθ2​​(y)​ Where $$\\phi_{\\theta}(x)$$ denotes the normal density with parameters $$\\theta = (\\mu,\\sigma^{2})$$. And the log-likelihood on the N training cases of data Z is given by l(θ;Z)=∑i=1Nlog[(1−π)ϕθ1(yi)+πϕθ2(yi)]\\begin{aligned} l(\\theta; Z) = \\sum_{i=1}^{N} log [ (1 - \\pi) \\phi_{\\theta_{1}}(y_{i}) + \\pi \\phi_{\\theta_{2}}(y_{i}) ] \\end{aligned} l(θ;Z)=i=1∑N​log[(1−π)ϕθ1​​(yi​)+πϕθ2​​(yi​)]​ Direct maximization of $$l(\\theta; Z)$$ is not feasible numerically therefore we proceed it in an iterative way. A more dedicated deduction is available in the reference book. In short, what we do here is to estimate π\\piπ by γi(θ)=Pr(Δi=1∣θZ)\\begin{aligned} \\gamma_{i}(\\theta) =Pr(\\Delta_{i} = 1 | \\theta Z) \\end{aligned} γi​(θ)=Pr(Δi​=1∣θZ)​ γi(θ)\\gamma_{i}(\\theta)γi​(θ) is also called the responsibility of model 2 for observation i. The EM algorithm consists of three steps: Initial guesses for the parameters $$\\hat{\\mu_{1}}$$, $$\\hat{\\sigma_{1}}^{2}$$, $$\\hat{\\mu_{2}}$$, $$\\hat{\\sigma_{2}}^{2}$$, $$\\hat{\\pi}$$. Expectation Step: compute the responsibilities γi^=π^ϕθ2^(yi)(1−π^)ϕθ1^(yi)+π^ϕθ2^(yi),i=1,2,...,N\\begin{aligned} \\hat{\\gamma_{i}} = \\frac{\\hat{\\pi} \\phi_{\\hat{\\theta_{2}}}(y_{i})}{ (1 - \\hat{\\pi}) \\phi_{\\hat{\\theta_{1}}}(y_{i}) + \\hat{\\pi}\\phi_{\\hat{\\theta_{2}}}(y_{i})}, i = 1,2,...,N \\end{aligned} γi​^​=(1−π^)ϕθ1​^​​(yi​)+π^ϕθ2​^​​(yi​)π^ϕθ2​^​​(yi​)​,i=1,2,...,N​ Maximization Step: : Update the means and variances: μ1^=∑i=1N(1−γi^)yi∑i=1Nγi^μ2^=∑i=1Nγi^yi∑i=1Nγi^σ1^=∑i=1N(1−γi^)(yi−μ1^)2∑i=1N(1−γi^)σ2^=∑i=1Nγi^(yi−μ2^)2∑i=1Nγi^π^=∑i=1Nγi^N\\begin{aligned} \\hat{\\mu_{1}} &amp;= \\frac{\\sum_{i=1}^{N} (1 - \\hat{\\gamma_{i}})y_{i}}{\\sum_{i = 1}^{N}\\hat{\\gamma_{i}}} \\\\ \\hat{\\mu_{2}} &amp;= \\frac{\\sum_{i=1}^{N} \\hat{\\gamma_{i}}y_{i}} {\\sum_{i = 1}^{N}\\hat{\\gamma_{i}}} \\\\ \\hat{\\sigma_{1}} &amp;= \\frac{\\sum_{i=1}^{N} (1 - \\hat{\\gamma_{i}})(y_{i}-\\hat{\\mu_{1}})^{2}}{\\sum_{i = 1}^{N}(1 -\\hat{ \\gamma_{i}})} \\\\ \\hat{\\sigma_{2}} &amp;= \\frac{\\sum_{i=1}^{N} \\hat{\\gamma_{i}}(y_{i}-\\hat{\\mu_{2}})^{2}}{\\sum_{i = 1}^{N}\\hat{ \\gamma_{i}}} \\\\ \\hat{\\pi} &amp;= \\sum_{i = 1}^{N}\\frac{\\hat{\\gamma_{i}}}{N} \\end{aligned} μ1​^​μ2​^​σ1​^​σ2​^​π^​=∑i=1N​γi​^​∑i=1N​(1−γi​^​)yi​​=∑i=1N​γi​^​∑i=1N​γi​^​yi​​=∑i=1N​(1−γi​^​)∑i=1N​(1−γi​^​)(yi​−μ1​^​)2​=∑i=1N​γi​^​∑i=1N​γi​^​(yi​−μ2​^​)2​=i=1∑N​Nγi​^​​​ Iterate step 2 and 3 until convergence. A Scratch of Mixture model Now, let us proceed and grasp the concepts mentioned above more clearly with code and data. (1) Implement a function to simulate data from a univariate mixture model by mixture model as mentioned above. r_uni_two_com = function(n, theta){ # simulate y1 and y2 according to the equations mentioned above y1 = rnorm(n,theta$mu_1,theta$sigma_1) y2 = rnorm(n,theta$mu_2,theta$sigma_2) # mixed y = (1 - theta$pi) * y1 + theta$pi * y2 return(y) } (2) Simulate 200 observations using $$\\mu_1=-2$$, $$\\mu_2=1.5$$, $$\\sigma_1=2 $$, $$ \\sigma_2=1 $$ and $$\\pi=0.3$$. Visualize observations in a histogram. n = 200 theta_0 = list(mu_1 = -2, mu_2 = 1.5, sigma_1 = 2, sigma_2 = 1, pi = 0.3) set.seed(2020) simulation_data = r_uni_two_com(n,theta_0) hist(x = simulation_data, main = &quot;Histgram of Simulated Data of Mixture Model&quot;, xlab = &quot;Mixed Y&quot;) (3) Compute the density value for a given set of parameters and data with a function $$d_uni_two_comp(x, \\theta)$$. # get the density by using dnorm() function d_uni_comp = function(x,theta){ density_m1 = dnorm(x,theta$mu_1,theta$sigma_1) density_m2 = dnorm(x, theta$mu_2,theta$sigma_2) return(list(density_m1 = density_m1,density_m2 = density_m2 )) } library(reshape2) # for merging the density plots # melt the density into one column so that we can plot them in one figure data&lt;- melt(d_uni_comp(simulation_data,theta_0) ) # combine the it with simulation data data = cbind(simulation_data,data) ggplot(data,aes(x=simulation_data,y = value, color=L1)) + geom_line(alpha=0.7)+ theme_minimal() + ggtitle(&quot;Simutated Data versus Corresponding Density&quot;) + theme(plot.title = element_text(hjust = 0.5),legend.title = element_blank()) + coord_cartesian(xlim =c(-4,4)) Model 1 (red line) seems to be centered around -2 and model 2 (blue line) seems to be centered around 2. It looks reasonable according to the data generation process. (4) Calculate $$\\gamma$$ at the expectation step. Here the function $$e_uni_two_comp(X, theta)$$ returns a vector of gamma for each row in X. # use equation above to calculate the resposibility e_uni_two_camp = function(x,theta){ gamma_i =( theta$pi * d_uni_comp(x,theta)$density_m2 ) / ( (1-theta$pi) * d_uni_comp(x,theta)$density_m1 + theta$pi * d_uni_comp(x,theta)$density_m2 ) return(gamma_i) } # initial guess of parameters theta_0 = list(mu_1 = 4.12, mu_2 = 0.94, sigma_1 = 2, sigma_2 = 2, pi = 0.5) gamma = e_uni_two_camp(y,theta_0) head(gamma) ## [,1] ## [1,] 0.9106339 ## [2,] 0.8716861 ## [3,] 0.7797225 ## [4,] 0.6645640 ## [5,] 0.6484311 ## [6,] 0.5178799 Initially, $$\\mu_1$$ = 4.12, $$\\mu_2$$ = 0.94, $$\\sigma_1$$ = 2, $$\\sigma_2$$ = 2. According to the definition of responsibility, the larger responsibility ( $$\\gamma$$ ) of a given x, the more likely it is belong to model 2, otherwise it may more likely to belong to model 1. For example, we can see that the first 6 observations, the responsibilities are bigger than 0.5, so they are likely belong to model 2. However, we need to continue on iterating the process until it converges. (5) Implement a function called $$max_uni_two_comp(X, gamma)$$ that returns a list with parameters mu1, mu2, sigma1, sigma2 and pi at the maximization step. max_uni_two_comp = function(y,gamma){ mu_1 = sum( (1 - gamma) * y ) / sum( 1 - gamma) mu_2 = sum( ( gamma) * y ) / sum( gamma) sigma_1 = sum( (1 - gamma) * ((y-mu_1)^2) ) / sum( 1 - gamma) sigma_2 = sum( ( gamma) * (y-mu_2)^2 ) / sum( gamma) pi = sum(gamma)/ length(y) return(list(mu_1 = mu_1, mu_2 = mu_2, sigma_1 = sqrt(sigma_1), sigma_2 = sqrt(sigma_2), pi = pi)) } theta = max_uni_two_comp(y,gamma) theta ## $mu_1 ## [1] 3.842941 ## ## $mu_2 ## [1] 1.450413 ## ## $sigma_1 ## [1] 1.700666 ## ## $sigma_2 ## [1] 1.47168 ## ## $pi ## [1] 0.4883709 These are the estimated parameters after the first iteration. (6) Now we need to implement the log-likelihood of the model as $$ll_uni_two_comp(x, theta)$$. Log-likelihood is important when evaluating the algorithm. ll_uni_two_camp = function(x,theta){ ll_i =sum( log( ( (1 - theta$pi) * d_uni_comp(x,theta)$density_m1 ) + (theta$pi * d_uni_comp(x,theta)$density_m2) ) ) return(ll_i) } ll_uni_two_camp(y,theta_0) ## [1] -43.1055 (7) Combine the implemented functions to an EM algorithm $$em_uni_two_comp(X, theta_0, iter)$$ that takes in a $$n \\times p$$ data matrix X and an initialization value for as $$theta_0$$. em_uni_two_comp = function(y,theta){ iter = 3 for (i in 1:iter) { gamma = e_uni_two_camp(y,theta) theta = max_uni_two_comp(y,gamma) ll = ll_uni_two_camp(y,theta) # print the log-likehood cat(&quot;Log Lik:&quot;,ll,&quot;\\n&quot;) } } em_uni_two_comp(y,theta_0) ## Log Lik: -41.53247 ## Log Lik: -41.11211 ## Log Lik: -40.48348 Here the iteration is set to be 3 and we can see that the log likelihood is getting smaller which is as expected. Test the algorithm on the y for 20 iterations and get the $$\\hat{\\pi}$$. for (iter in c(1,5,10,15,20)) { # iter = 20 # intial gamma gamma = e_uni_two_camp(y,theta_0) # start the iteration of EM for (i in 1:iter) { theta = max_uni_two_comp(y,gamma) ll = ll_uni_two_camp(y,theta) gamma = e_uni_two_camp(y,theta) } cat(&quot;iteration=&quot;,iter,&quot;;&quot;,&quot;estimated pi = &quot;,theta$pi, &quot;\\n&quot;) } ## iteration= 1 ; estimated pi = 0.4883709 ## iteration= 5 ; estimated pi = 0.4981389 ## iteration= 10 ; estimated pi = 0.5436594 ## iteration= 15 ; estimated pi = 0.5532677 ## iteration= 20 ; estimated pi = 0.5544302 Evaluation Next we can evaluate EM algorithm by plotting the change of log likeliood to see if it converges. estimated_final_ll =c() iter = 20 # intial gamma gamma = e_uni_two_camp(y,theta_0) # start the iteration of EM for (i in 1:iter) { theta = max_uni_two_comp(y,gamma) ll = ll_uni_two_camp(y,theta) gamma = e_uni_two_camp(y,theta) estimated_final_ll = cbind(estimated_final_ll,ll) } plot(estimated_final_ll[1,], ylab = &quot;observed Data Log-Likelihood&quot;, xlab = &quot;iteration&quot;, main = &quot;Estimated Log-likelihood over Iterations&quot;) The trend looks converged after iteration = 10. Therefore the algorithm is working well for the given dataset. Run the EM algorithm with the other dataset. Here I set the iteration to be 20. The estimated parameters will be printed under the code. Eruptions variable of Faithful dataset: erruptions = faithful$eruptions # initial guess of parameters theta_erup = list(mu_1 = 1.75, mu_2 = 4.5, sigma_1 = 1, sigma_2 = 1, pi = 0.55) estimated_final_ll =c() iter = 20 # intial gamma gamma = e_uni_two_camp(erruptions,theta_erup ) # start the iteration of EM for (i in 1:iter) { theta_faithful = max_uni_two_comp(erruptions,gamma) ll = ll_uni_two_camp(erruptions,theta) gamma = e_uni_two_camp(erruptions,theta) } theta_faithful ## $mu_1 ## [1] 4.256629 ## ## $mu_2 ## [1] 2.05479 ## ## $sigma_1 ## [1] 0.4899172 ## ## $sigma_2 ## [1] 0.3383707 ## ## $pi ## [1] 0.3491834 Petal.Length variable of Iris dataset: pd_length = iris$Petal.Length estimated_final_ll =c() # set the initial parameters theta_iris = list(mu_1 =1, mu_2 = 4, sigma_1 = 1, sigma_2 = 1, pi = 0.5) iter = 20 # intial gamma gamma = e_uni_two_camp(pd_length,theta_iris ) # start the iteration of EM for (i in 1:iter) { theta_iris = max_uni_two_comp(pd_length,gamma) ll = ll_uni_two_camp(pd_length,theta) gamma = e_uni_two_camp(pd_length,theta) } theta_iris ## $mu_1 ## [1] 4.919392 ## ## $mu_2 ## [1] 1.502265 ## ## $sigma_1 ## [1] 0.8159113 ## ## $sigma_2 ## [1] 0.3266122 ## ## $pi ## [1] 0.3398739 Visualize the density for the two datasets using the parameters estimated with EM algorithm. First visualize for the faithful dataset data&lt;- melt(d_uni_comp(erruptions,theta_faithful)) # combine the it with simulation data data = cbind(erruptions,data) ggplot(data,aes(x=erruptions,y = value, color=L1)) + geom_line(alpha=0.7)+ theme_minimal() + ggtitle(&quot;Density for Erruptions after Estimated by EM Algorithm&quot;) +theme(plot.title = element_text(hjust = 0.5),legend.title = element_blank()) EM estimated density of faithful data Then visualize for the iris dataset data&lt;- melt(d_uni_comp(pd_length,theta_iris)) # combine the it with simulation data data = cbind(pd_length,data) ggplot(data,aes(x=pd_length,y = value, color=L1)) + geom_line(alpha=0.7)+ theme_minimal() + ggtitle(&quot;Density for Pedal Length after Estimated by EM Algorithm&quot;) +theme(plot.title = element_text(hjust = 0.5),legend.title = element_blank()) EM estimated density of iris data These clustering results looks reasonable. Great! Conclusion This post shared how to derive the basic pieces of EM algorithm in the two-component mixture model case. We can see in the end the algorithm gives us the a mixture distribution based on the given dataset instead of telling directly which observations belong to which cluster. In the future I will share how to use EM algorithm in general. Thank you for reading! ","link":"https://BolinWu-Gridea.github.io/post/2021-02-03-EM_Post/"},{"title":"Data Visualization with ggplot2","content":"Introduction In this post I will share some frequently used ggplot2 commands when making data visualization. To make it easy to replicate, I will use the built-in iris and titanic dataset, which consists of numeric variable and categorical variable, for illustration. Load the data and package library(ggplot2) library(tidyverse) data(iris) data(&quot;Titanic&quot;) df = iris df_titanic = as_data_frame(Titanic) # take a look at iris head(df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa head(df_titanic) ## # A tibble: 6 x 5 ## Class Sex Age Survived n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1st Male Child No 0 ## 2 2nd Male Child No 0 ## 3 3rd Male Child No 35 ## 4 Crew Male Child No 0 ## 5 1st Female Child No 0 ## 6 2nd Female Child No 0 Scatter plots We can use ggplot to create the coordinate system and use geom_point() to add a layer of points to the graph. ggplot(data = df) + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width)) We can use size to change the dots size. Use color and shape to make the scatter plots more informative. # change the size ggplot(data = df) + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), size = 1) # add color and shape ggplot(data = df) + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width, shape = Species, color = Species)) Sometimes we may only interested in visualizing a data in certain scale. It can be easily done with the help of filter function in tidyverse package. # visualiza data in certain scale ggplot(data = filter(df, Sepal.Length &gt; 5)) + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width), size = 1) If we want to group by categories, we simply add facet_wrap() function. # group by category ggplot(data = df) + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width,color = Species), size = 1) + facet_wrap(~Species) Bar charts Here we will start to use titanic dataset. A basic bar chart can be made like this: ggplot(df_titanic) + geom_bar(aes(x = Age)) By applying fill a tacked bar can be created: ggplot(df_titanic) + geom_bar(aes(x = Age, fill = Survived) ) If we do not want the bar to be stacked: ggplot(df_titanic) + geom_bar(aes(x = Age, fill = Survived), position = &quot;dodge&quot; ) Customization: Titles and labels A formal plot needs to have proper titles, axis label and legend titles, etc. These can be set by using labs: bar_titanic = ggplot(df_titanic) + geom_bar(aes(x = Age, fill = Survived) ) bar_titanic + labs(title = &quot;Survival Age&quot;, subtitle = &quot;Same survival number of adult and child&quot;, caption = &quot;Source: R built-in dataset&quot;, x = &quot;Passenger age&quot;, y = &quot;Number&quot;, fill = &quot;Survived or not&quot;) Customization: Scales Scales can map values in the data space to the “aesthetic space”. It allows us to adjust the plot aesthetically. bar_titanic + scale_fill_discrete(labels = c(&quot;Did not survived&quot;, &quot;Survived&quot;)) Note that in the example, fill = Survived which is a discrete variable that is why we use scale_fill_discrete. Otherwise we may use scale_fill_continuous() if it is a continuous data or scale_fill_date() if it is date data./ The colors can be changed manually: bar_titanic + scale_fill_manual(labels = c(&quot;Did not survived&quot;, &quot;Survived&quot;), values = c(&quot;grey&quot;, &quot;blue&quot;)) Please be cautious that many times we need to use color-blind friendly color palette. So instead of using value = “color name”, we should use the pre-set palette: # The palette with grey: cbPalette &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) bar_titanic + scale_fill_manual(labels = c(&quot;Did not survived&quot;, &quot;Survived&quot;), values = c(cbPalette[1], cbPalette[2])) ggplot(data = df) + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width)) + scale_y_continuous(breaks = c(2,3,4), labels = c (&quot;2&quot;,&quot;3&quot;, &quot;4&quot;))+ labs(title = &quot;Set y-axis to be looser&quot;) ggplot(data = df) + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width)) + scale_y_continuous(breaks = seq(0,5,0.1))+ labs(title = &quot;Set y-axis to be more compact&quot;) Other resources for reference ggplot 2 is a powerful package and it is constantly evolving, there are some useful resouces online: http://www.cookbook-r.com/Graphs/ shows lots of graphing basics. https://exts.ggplot2.tidyverse.org/gallery/ gives some fancy extensions. In the end I would like to say that in the real word, it is more time consuming to clean the data than to visualize them therefore it is important to learn how to impute the dataset as well. And the best way to master data visualization is to learn what we need when encountering problems. Thank you for reading! ","link":"https://BolinWu-Gridea.github.io/post/2021-01-31-Useful-Commands-of-ggplot2-/"},{"title":"About","content":" 欢迎来到我的小站呀，很高兴遇见你！🤝 Welcome! ","link":"https://BolinWu-Gridea.github.io/post/about/"}]}