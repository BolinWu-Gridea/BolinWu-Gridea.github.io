{"posts":[{"title":"An Overview of Causal Inference (Part 1: Causal Effects and Confounding)","content":"Introduction Causal inference has been a heated field in statistics. It has great application for observational data. In this post I will shares some key concepts of causal inference: The confusion over causal inference The important causal assumptions The concept of causal effects Confounding and Directed Acyclic Graphs Main reference: A Crash Course in Causality: Inferring Causal Effects from Observational Data from Coursera. Confusion over causality Spurious correlation Causally unrelated variables might happen to be highly correlated with each other over some period of time. E.g. Divorce rate in Marine correlates with Percapita consumtion of margarine. Anecdotes Example: Bill Smith lived to be 105 years old. He said the secret to his longevity was eating one turnip a day. All we know is that Bill Smith lived to be 105 years old AND he ate one turnip a day. We do not know if eating turnips contributed to his lifespan. We do not know what would happen if other people adopted this habbit. Science reporting Headlines often do not use the forms of the word cause, but do get interpreted causally. Example: Positive link between video games and acedemic performance, study suggests. In reality, a lot of times how skeptically people view headlines depends on what their prior beliefs are. In causality analysis, we want to move away from that to a large degree. Instead, we want to look at the evidence as it is. Some key points: What statistical methods did they use? How was the study designed? What assumptions did they made? Reverse causality Even if there is a causal relationship, sometimes the direction is unclear. Example: Urban green space and exercise. Does green space in urban environments cause people to exercise more? Or the fact that more people come to exercise causes the govenment to build more gree space? How to clear up confusion? The dield of causal inference attempts to do this by proposing: Formal definitions of causal effects. Assumptions necessary to identify causal effects from data. Rules about what variables need to be controlled for. Sensitivity analysis to determine the impact of violations of assumptions on conclusions. Statisticians started working on causal modeling as far back as the 1920s (Wright 1921; Neyman 1923) It became its own area of statistical research since around 1970s. Some highlights: Re-introduction of potential outcomes: Rubin causal model (Rubin 1974). Causal diagrams (Greenland and Robins 1986; Pearl 2000). Propensity scores (Rosenbaum and Rubin 1983). Time-dependent confounding (Robins 1986; Robins 1997). Optimal dynamic treatment strategies (Murphy 2003; Robins 2004). Target learning (vander Laan 2009). As we dive deeper into causal modeling, it will be important to remember: Causal inference requires making some untestable assumptions (reffered to as causal assumptions) Cochran (1972) concludes: &quot;...observational studies are an interesting and challenging field which demands a good quality of humility, since we can claim only to be groping toward the truth.&quot; Treatment, potential outcomes and counterfactuals Here we will introduce some notations that is important for the following post. Suppose we are interested in the causal effect of some treatment A on some outcome Y. treatment example: A = 1 if receive influenza vaccine; A = 0 otherwise. Here is a treatment that takes two values 1 or 0. Outcome example: Y = 1 if develop cardiovascular disease within 2 years; Y = 0 otherwise. What is potential outcomes? You can think of it as the possible outcomes before the study takes place. Notation: $$Y^{a}$$ is the outcome that would be observed if treatment was set to A = a. What about counterfactuals? Counterfactual outcomes are ones that would have been ovserved had the treatment been different. For example: if my treatment wes A = 1, then my counterfactual outcome is $$Y^{0}$$. Hypothetical intervention One important assumption of causal effects of intervention is that the variables can be manipulated. Halland (1986) famously wrote &quot;no causation without manipulation&quot;&quot;. We can imagine that we can manupulate some people get drug A while others get drugs B. However, it is less clear about what a causal effect of an immutable variable would mean e.g. gender, age, race. One way to approch this is to relate these varibales to the variables that we can manupulate. No direct intervention Manipulable intervention Race Name on resume Obesity Bariatric surgery Socioeconomic status Gift of money For the remainder of the post, we will primarily focus on treaments that could be thought of as interventions. Treatments that we can imagine being randomized (manupulated) in a hypothetical trial. The reason that we focus on causal effect of hypothetical interventions is that Their meaning is well defined. They are potentially actionable. What are causal effects? In general: A had a causal effect on Y if Y1Y^{1}Y1 differs from Y0Y^{0}Y0. The foundamental problem of causal inference is that we can only observe one potential outcome for each person. However, with certain assumptions, we can estimate pupulation level (average) causal effects. That is, rather than think if the causal effect work for individual, we think of the population as a whole. Therefore we never know the unit level causal effect. Hopeless: What would have happened to me had I not taken ibuprofen? (unit elvel causal effect) Possible: What would the rate of headache remission be if everyone took ibuprofen when they had a headache versus no one did? Average ausal Effect Definition: E(Y1−Y0)E(Y^{1} - Y^{0})E(Y1−Y0). It means the average value of Y if everyone was treated with A = 1 minus the average value of Y if everyone was treated with A = 0, if Y is binary. Please note that this is just an ideal definition because we could never actually observe that in the real world. Conditioning on, VS setting, treatment In general, E(Y1−Y0)≠E(Y∣A=1)−E(Y∣A=0)\\begin{aligned} E(Y^{1} - Y^{0}) \\neq E(Y|A = 1) - E(Y|A = 0) \\end{aligned} E(Y1−Y0)​=E(Y∣A=1)−E(Y∣A=0)​ The reason is that $$E(Y^{1}$$ is the mean of Y if the whole population was treated with A = 1; while $$E(Y|A = 1)$$ is mean of Y among people with A = 1. Technically, $$E(Y|A = 1) - E(Y|A = 0)$$ is not a causal effect because it is comparing two different populations of people. Other causal effects Other causal effects that we may be interested in are: E(Y^{1} / Y^{0})$$ : causal relative risk Challenge How do we use observed data to link observed outcomes to potential outcomes? What assumptions are necessay to estimate causal effects from observed data? Causal assumptions Identifiability of causal effects requires making some untestable assumptions. These are generally called causal assumptions. The most common are : Stable Unit Treatment Value Assumption (SUTVA) Consistency Ignorability Positivity They are all about the observed data: Y, A and a set of pre-treatment covariates X. SUTVA It involves two assumptions: No interference: Unites do not interfere with each other Treatment assigment of one unit does not affect that outcome of another unit. &quot;Spoillover&quot; or &quot;contagion&quot; are also terms for interference. One version of treatment The potential outcomes can effectively linked to the observed data. SUTVA allows us to write potential outcome for the ith person in terms of only that person's treatments. Consistency assumption The potential outcome under treatment A = a is equal to the observed outcome if the actual treatment received is A = a. Ignorability assumption Given pre-treatment covariates X, treatment assignment is independent from the potential outcomes. Y0,Y1∐A∣XY^{0}, Y^{1} \\coprod A|X Y0,Y1∐A∣X Essentially it means that the treatment A is randomly assigned regardless of X. Positivity assumtion It refers to that everybody has a positive opportunity to receive either treatment. P(A=a∣X=x)&gt;0,for all a and xP(A = a| X = x) &gt;0 , \\text{for all a and x} P(A=a∣X=x)&gt;0,for all a and x If treatment was deterministic for some values of X, then we would have no ovserved values of Y for one of the treatment groups for thoses values of X. Confounding and Directed Acyclic Graphs (DAGs) Confounding control Confounders are often defined as variables that affect treatment and affect the outcome. We are interested in identifying a set of variables X that will make the ignorability assumption hold. Then we want to use statistical methods, which will be covered later in the course, to control these variables and estimated causal effects. Causal graph Graphs (causal graphs or directed acyclic graphs) are considered useful for causal inference. The functions of causal graphs are: Helpful for identifying which variables to control for Make assumptions eplicit. Here I would not explain all the details about DAGs, more information about compatibility between DAGs and distributions can be found here. Instead, I would like to note down some interesting facts when learning DAGs path and associations. For a fork path A&lt;−E−&gt;BA &lt;-E-&gt; BA&lt;−E−&gt;B, A and B are dependent because the information from E flows to both A and B. For colliders A−&gt;G&lt;−BA-&gt;G&lt;-BA−&gt;G&lt;−B, A and B are independent. However, if we control forff G, then A and B are dependent. Ending Now we have an overview of the causal effects and we know that DAGs is an important method to identify the variables need to be controlled in order to achieve ignorability assumption. Next we will proceed to see how to control the counfounders. The two of the general approaches are matching and inverse probability of treatment weighting. ","link":"https://BolinWu-Gridea.github.io/post/2021-04-20-CausalityIntroductionP1/"},{"title":"An Overview of Causal Inference (Part 2: Propensity Score and Matching)","content":"Introduction In the Part 1 we talked about the basic concepts of causal effect and confounding. In this post we will proceeed with discussing about how to control the confounders with matching. The main content consists of: The concept of matching. The concept of propensity score. Demonstrate how to match on propensity score in R. Analyze the outcome after matching. Matching Randomized trials In a randomized trial, treatment assignment A would be determined by a coin toss, in order to erase the effect of X to A. It is a good way to get rid of confounding. However, the researchers may not always use randomized trials for the following reasons: Randomized trials are expensive. Sometimes randomizing treatment/exposure is unethical. Some people may refuse to participate in trials. Randomized trials take time. And that brings us the observational studies. Observational studies are planned, prospective, observational studies with active data collection: Like trials: data collected on a common set of variables at planned times; outcomes carefully measured; study protocols. Unlike trials: regulations much weaker, since not intervening; broader population eligible for the study. What is matching? Matching is a method that attempts to make an observational study more like a randomized trial. Main idea: Match individuals in the treated group A = 1 to individuals in the control group A = 0 on the covariates X. Advantages of matching: Controlling for confounders is achieved at the design phase. Matching will reveal lack of overlap in covariate distribution. The positivity assumption will hold in the population that can be matched. Once data are matched, essentially treated as if from a randomized trial. Fine balance Sometimes it is difficult to find great matches. We may be willing to accept some non-ideal matches if treated and control groups have same distribution of covariates. For example: Match 1: treated, male, age 40 and control, female, age 45 Match 2: treated, female, age 45 and control, male, age 40 We can see that the neighter match 1 nor match 2 is a good match. However, if we look at them together, we will find that the average age and percent of male/female are the same. Therefore it is a fine balance. There is a lot of software that can impose fine balance. How to match? Since in real data, we typically can not match exactly, we first need to choose some metric of closeness. There are two common options: Mahalanobis distance Robust Mahalanobis distance Mahalanobis distance is defined as follows. Denote the value of a vector of covariates for subject j by XjX_{j}Xj​ and the covariance matrix by S. D(Xi,Xj)=(Xi−Xj)TS−1(Xi−Xj)D(X_{i}, X_{j}) = \\sqrt{(X_{i} - X_{j})^{T}S^{-1}(X_{i} - X_{j})}D(Xi​,Xj​)=(Xi​−Xj​)TS−1(Xi​−Xj​)​ Robust Mahalanobis distance is aiming at solving the outliers problem. Outliers can creat large distances between subjects, even if the covariates are otherwise similar. Its application is as follows. Replace each covariate value with its rank. For example, we set the rank of the youngest age as 1, the second youngest as 2, etc. So the difference between the youngest and the second youngest is only 1. Constant diagonal on covariance matrix. Now ranks should all be on the same scale now, so that we do not want to weigh one variable more than the other. Calculate the usual Mahalanobis distance on the ranks. Greedy (nearest-neighbor) matching Steps: Randomly order list of treated subjects and control subjects. Start with the first treated subject. Match to the control with the smallest distance. This is called greedy. Remove the matched control from the list of available matches. Move on to the next treated subject. Match to the control with the smallest distance. Repeat steps 3 and 4 until you have matched all treated subjects. In R, this can be done by R package &quot;MatchIt&quot;. Caliper We might prefer to exclude treated subject for whom there does not exist a good match. A bad match can be defined by using a caliper --- maximum acceptable distance. Main idea: Only matcha treated subject if the best match has distance less than the caliper. If no matches within caliper, it is a sign that positivity assumption would be violated. Excluding these subjects makes assumption more realistic. Optimal matching Why greedy matching is not optimal? Greedy matching does not lead to the smallest total distance. The optimal matching minizes global distance measure. It is conputationally demanding. Whether it is feasible to perform depends on the size of the problem, which is measured by the number of posdsible treatment-control pairings. R package &quot;optmatch&quot; and &quot;rcbalance&quot; can do the work. Consatraints can be imposed to make optimal matching computationally feasible for larger data sets. And that brings us to the sparse optimal matching. Sparce optimal matching We could match treated and control subjects within primary disease category. Or we focus on achieving optimal matching withing a subgroup of the treated or control subjects. Also, we can tolarate mismatches as long as fine balance can still be achieved. Assessing balance After we have matched, we should access whether matching seemed to work. We can look at a couple of things. Covariate balance standardied differemces. smd=Xˉtreatment−Xˉcontrol(streatment2+scontrol2)/2smd = \\frac{\\bar{X}_{treatment} - \\bar{X}_{control}}{\\sqrt{(s_{treatment}^{2} + s_{control}^{2}) / 2 }}smd=(streatment2​+scontrol2​)/2​Xˉtreatment​−Xˉcontrol​​ Wheter the means between the two groups are similar? Hypothesis tests: Test for a difference in means between treated and controls for each convariate. Two sample t-tests Report p-value for each test. Please note that the p-value is dependent on the sample size. Analyzing data after matching After successfully matching and achieving adequate balance, proceed with outcome analysis. Randomization tests The main idea is as follows: Compute test statistics from observed data Assume null hypothesis of no treatment effect is true Randomly permute treatment assignment within pairs and recompute test statistic. Repeat many times and see how unusual observed statistic is. This procedure is equivalent to the McNemar test for paried data or paired t-test for continuous data. Sensitivity analysis The previous mentioned matching aims to achieve balance on observed covariates. If there was imbalance on observed covariates, we call it overt bias. However, if there was imbalance on the variables that we did not match on (including unobserved variables), then we have hidden bias. Then the ignorability assumption violated. The main idea of sensitivity analysis is that if there is hidden bias, determine how severe it would have to be change conclusions. Let $$\\pi_{j}$$ be the probability that person j receives treatment. πk\\pi_{k}πk​ be be the probability that person k receives treatment. Suppose person j and k are perfectly matched, so that their observed covariates XjX_{j}Xj​ and XkX_{k}Xk​ are the same. If πj\\pi_{j}πj​ = πk\\pi_{k}πk​, then there is no hidden bias. Consider the follow following inequality 1Γ≤πj/(1−πj)πk/(1−πk)≤Γ\\frac{1}{\\Gamma} \\leq \\frac{\\pi_{j}/(1 - \\pi_{j})} {\\pi_{k} / (1 - \\pi_{k})} \\leq \\GammaΓ1​≤πk​/(1−πk​)πj​/(1−πj​)​≤Γ. Γ\\GammaΓ is the odds ratio. If Γ\\GammaΓ = 1, then no overt bias. If Γ\\GammaΓ &gt;1 implies hidden bias. Propensity score The propensity score is the probability of receiving treatment, rather than control, given covariates X. If we define A =1 for treatment and A = 0 for control. We will denote the propensity score for subject i by πi\\pi_{i}πi​. πi=P(A=1∣Xi)\\pi_{i} = P(A = 1|X_{i}) πi​=P(A=1∣Xi​) If a person i has a propensity score value of 0.3, that means, given their particular covariate values, there is a 30% chance they will be treated. Balancing score Suppose two subjects have the same value of the propensity score, but they possibly have different covariate values of X. Despite the different covariate values, they were both equally likely to have been treated. This means that both subjects X is just as likely to be found in the treatment group. If you restrict to a subpopulation of subjects who have the same value of the propensity score, there sould be balance in the two treatment groups. Then the propensity score is a balancing score. More formally, P(X=x∣π(X)=p,A=1)=P(X=x∣π(X)=p,A=0)P(X=x|\\pi(X) = p, A = 1) = P(X=x|\\pi(X) = p, A = 0) P(X=x∣π(X)=p,A=1)=P(X=x∣π(X)=p,A=0) Implication: if we match on the propensity score, we should achieve balance. Another way of thinking this is that, considering we assumed ignorability - that treatment is randomized given X. Then conditioning on the propensity score is the same as conditioning on an allocation probability. Estimated propensity score In a randomized trial, the propensity score is generally known. However, in an observational study, it will be unknown. Notice that the propensity score involves observed data: A and X. Therefore we can estimate it. Typically when people talk about a propensity score, they are referring to the estimated propensity score. We need to estimate $$P(A=1|X)$$ with outcome to be A. If A is binary, we can estimate the propensity score by using logistic regression. Propensity score matching Trimming tails If there is a lack of overlap, trimming the tails is an option. This means removing subjects who have extreme values of the propensity score. For example, removing control subjects whose propensity score is less than the minimum in the treatment group. Trimming can make the positivity assumption more reasonable. Matching We can proceed by computing a distance between the propensity score for each treated subject with every control. Then we use the nearest neighbor or optimal matching as before. In practice, logit of the propensity score is often used, rather than the propensity score itself. The reason is that the propensity score is bounded between 0 and 1, making many values similar. However, the logit of the propensity score is unbounded. This transformation essentially stretches the distribution, while preserving ranks. To ensure that we do not accept any bad matches, a caliper can be used. In practice, a common choice for a caliper is the 0.2 times the standard deviation of logit of the propensity score. The procedure will be like as follows: Estiamte the propensity score (e.g. using logistic regression). Logit-transform the propensity score. Take the standard deviation of this transformed variable. Set the caliper to 0.2 times the value from step 3. Please note that the 0.2 can be set to an arbitrary number. This propensity score matching can be done by R package MatchIt. Analyze data in R using propensity score matching we will use data from Lalonde (1986), that aimed to evaluate the impact of National Supported Work (NSW) Demonstration, which is a labor training program, on post-intervention income levels. Interest is in estimating the causal effect of this training program on income. Load package and data First load the packages tableone and Matching: # install.packages(&quot;tableone&quot;) # install.packages(&quot;Matching&quot;) # install.packages(&quot;MatchIt&quot;) library(tidyverse) library(tableone) library(Matching) library(MatchIt) data(lalonde) str(lalonde) The data have n=614 subjects and 9 variables: age: age in years. educ: years of schooling. race: indicator variable for races. hispan: indicator variable for Hispanics. married: indicator variable for marital status. nodegree: indicator variable for high school diploma. re74: real earnings in 1974. re75: real earnings in 1975. re78 real earnings in 1978. treat: an indicator variable for treatment status. The outcome is re78 – post intervention income. The treatment is treat – which is equal to 1 if the subject received the labor training and equal to 0 otherwise. The potential confounding variables are: age, educ, black, hispan, married, nodegree, re74, re75. Pre-matching examination Before mathching, let us find the standardized differences for all of the confounding variables. xvars = colnames(lalonde %&gt;% select(-c(&quot;treat&quot;,&quot;re78&quot;))) tabel1 = CreateTableOne(vars = xvars, strata = &quot;treat&quot;, data = lalonde, test = F) print(tabel1,smd = T) Stratified by treat 0 1 SMD n 429 185 age (mean (SD)) 28.03 (10.79) 25.82 (7.16) 0.242 educ (mean (SD)) 10.24 (2.86) 10.35 (2.01) 0.045 race (%) 1.701 black 87 (20.3) 156 (84.3) hispan 61 (14.2) 11 ( 5.9) white 281 (65.5) 18 ( 9.7) married (mean (SD)) 0.51 (0.50) 0.19 (0.39) 0.719 nodegree (mean (SD)) 0.60 (0.49) 0.71 (0.46) 0.235 re74 (mean (SD)) 5619.24 (6788.75) 2095.57 (4886.62) 0.596 re75 (mean (SD)) 2466.48 (3292.00) 1532.06 (3219.25) 0.287 Q1. What is the standardized difference for married (to nearest hundredth)? Answer: From the printed table above we can see that the standardized difference for married is nearly 0.719. Q2. What is the raw (unadjusted) mean of real earnings in 1978 for treated subjects minus the mean of real earnings in 1978 for untreated subjects? re78_mean = lalonde %&gt;% group_by(treat) %&gt;% summarise_at(&quot;re78&quot;, .funs = mean) (re78_mean %&gt;% filter(treat ==1) - re78_mean %&gt;% filter(treat ==0))$re78 # The difference is -635 Matching on propensity score (without caliper) Now let us estimated a propensity score model by using the logistic regression mdoel, where the outcome is treatment. Obtain the propensity score for each subject. psmodel = glm(treat~.-re78, family = binomial(),data = lalonde) summary(psmodel) # find the propneisty scrore pscore = psmodel$fitted.values Carry out propensity score by using the Match function psmatch = Match(Tr = lalonde$treat, M = 1, X = pscore, replace = F) matched = lalonde[unlist(psmatch[c(&quot;index.treated&quot;,&quot;index.control&quot;)]),] matchedtab1 = CreateTableOne(vars = xvars, strata = &quot;treat&quot;, data = matched, test = F) print(matchedtab1,smd = T) Stratified by treat 0 1 SMD n 185 185 age (mean (SD)) 25.22 (10.57) 25.82 (7.16) 0.066 educ (mean (SD)) 10.54 (2.72) 10.35 (2.01) 0.079 race (%) 0.855 black 87 (47.0) 156 (84.3) hispan 41 (22.2) 11 ( 5.9) white 57 (30.8) 18 ( 9.7) married (mean (SD)) 0.21 (0.41) 0.19 (0.39) 0.041 nodegree (mean (SD)) 0.64 (0.48) 0.71 (0.46) 0.139 re74 (mean (SD)) 2383.67 (4287.87) 2095.57 (4886.62) 0.063 re75 (mean (SD)) 1651.45 (2671.39) 1532.06 (3219.25) 0.040 From the column of SMD, we can see that the values are smaller. This is exactly what we want to see. Usually a boudnary value is 0.1. We would pay attention to the variates with SMD &gt; 0.1. If it is too big, then maybe there is imbalance. Q3. What is the standardized difference for married? Answer: It is reduced to 0.041. We end up with better matching. We can also try carrying out the propensity score matching using the matchit function. And with this function we can easily find the propensity overlapping. The default measure is logistic regression propensity scores. # use matchit for propensity score, nearest neighbor matching # the default measure is logistic regression propensity scores. m.out = matchit(treat~. - re78, data = lalonde, method = &quot;nearest&quot;) summary(m.out) # propensity score plots plot(m.out, type = &quot;hist&quot;) plot(m.out, type = &quot;jitter&quot;) Check overlap by jitter plot Check overlap by histogram Matching on propensity score (with caliper) Re-do the matching and set the caliper = 0.1. # re-do matching using a caliper psmatch = Match(Tr = lalonde$treat, M = 1, X = pscore, replace = F,caliper = 0.1) matched = lalonde[unlist(psmatch[c(&quot;index.treated&quot;,&quot;index.control&quot;)]),] matchedtab1 = CreateTableOne(vars = xvars, strata = &quot;treat&quot;, data = matched, test = F) print(matchedtab1,smd = T) Stratified by treat 0 1 SMD n 111 111 age (mean (SD)) 26.46 (11.39) 26.22 (7.18) 0.026 educ (mean (SD)) 10.39 (2.70) 10.25 (2.31) 0.054 race (%) 0.048 black 80 (72.1) 82 (73.9) hispan 11 ( 9.9) 11 ( 9.9) white 20 (18.0) 18 (16.2) married (mean (SD)) 0.22 (0.41) 0.24 (0.43) 0.064 nodegree (mean (SD)) 0.63 (0.48) 0.65 (0.48) 0.037 re74 (mean (SD)) 2545.63 (4347.60) 2250.49 (5746.14) 0.058 re75 (mean (SD)) 1740.06 (2725.04) 1222.25 (3081.19) 0.178 Q4. For the matched data, what is the mean of real earnings in 1978 for treated subjects minus the mean of real earnings in 1978 for untreated subjects? y_trt = matched$re78[matched$treat==1] y_con = matched$re78[matched$treat==0] mean(y_trt) - mean(y_con) [1] 893.4831 The difference is around 893. Q5. Carry out a paired t-test for the effect of treatment on earnings. What are the values of the 95% confidence interval? diffy = y_trt - y_con # paired t-test t.test(diffy) One Sample t-test data: diffy t = 1.0297, df = 110, p-value = 0.3054 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: -826.1391 2613.1052 sample estimates: mean of x 893.483 From the result we can not reject the null that there is no effect of treatment on earnings. Ending In this post we have introduced the powerful method of controling confounders, matching on propensity score. We also have demonstrated how to implement the analysis in R. In general, propensity score matching involves the following steps: Estimate propensity score; Match on propensity score; Check propensity score overlap; Check covariate balance My personal biggest take away after learning this is that, it is not difficult to perform matching, however, we need to be careful when evaluating the outcomes of matching. How does the overlapping of matched propensity score looks like? Whether the covariates are balanced? What does the treatment difference test tells us? These questions need to be throughly considered. Next we will continue to share how to estimate the causal effect by inverse probability of treatment weighting. Thank you for reading. ","link":"https://BolinWu-Gridea.github.io/post/2021-05-08-CausalityIntroductionP2/"},{"title":"Prediction of Children Anaemia Rate by LASSO","content":"Introduction This post investigates the five factors that are related to anaemia in children by using the data collected from the World Health Organization. The method we will use is LASSO, which is a classic penalized regression. In this post we will see how LASSO filter out the variable for us and its prediction performance compared with our baseline model, linear regression. To implement LASSO in R, the package I used is &quot;glmnet&quot;. Data Description The variables are downloaded from different sections and countries on the website and then merged together manually. The cleaned data set is available here. Please note that usually people need to do descriptive analysis of all the variables before start modelling. However in this post we are focusing on the LASSO implementation therefore it is not shown here. Independent Variable: Health Expenditure as percentage of GDP (%\\%%): This factor measures government’s health expenditurea as percentage of GDP. This is a main factor to evaluate government’s influence on the prevalence of anaemia. Antenatal care coverage - at least four visits (%\\%%): Antenatal care coverage (at least one visit) is the percentage of women aged 15–49 with a live birth in a given time period that received antenatal care provided by skilled health personnel at least once during their pregnancy. Prevalence of low birth weight (%\\%%): Low birth weight is defined as weight at birth less than 2500h (5.5 lb). Prevalence of anaemia in pregnant women (%\\%%): It measures the occurrence of anaemia happen among pregnant women. Nursing and midwifery personnel (per 10,000 population): Nurses and midwives include professional nurses, professional midwives, auxiliary nurses, auxiliary midwives, enrolled nurses, enrolled midwives and other associated personnel, such as dental nurses and primary care nurses. Dependent Variables Prevalence of anaemia in children (%\\%%): It measures the occurrence of anaemia happen among children who are under 6 months. All variables above are percentages/rate (per 10,000 population), so there is no measure of unit for each variable LASSO Intuition LASSO is one of the most popular regression analyses that perform variable selection in statistics and machine learning field. Different from linear regression, LASSO regression is biased but it provides a model with smaller variance, which makes it accurate in terms of doing prediction. The following equation is the expression of LASSO regression. min⁡βj(1N∑i=1n(yi−∑jpxijβj)2+λ∑j=1p∣βj)∣)\\begin{aligned} \\min_{\\beta_{j}}( \\frac{1}{N} \\sum_{i=1}^{n}(y_{i} - \\sum_{j}^{p} x_{ij} \\beta_{j})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_{j})| ) \\end{aligned} βj​min​(N1​i=1∑n​(yi​−j∑p​xij​βj​)2+λj=1∑p​∣βj​)∣)​ From the equation we can see that the regression can be basically divided into two parts. One part is the OLS linear regression part. The other part to the right is what we called the penalty term. The bigger $$\\lambda$$ is, the stronger restriction on $$\\beta$$ will be. Fig.1 - Visualization of LASSO in two-dimemsion situation The Figure 1 above helps us better understand how the LASSO works in the two-dimension scenario. Notice that the point where the red circle touches the blue square, β1\\beta_{1}β1​ equals zero. It means β1\\beta_{1}β1​ is removed from our model. At the same time, on that point we can acquire the LASSO model with the smallest mean squared error. Implementation in R With the help of &quot;glmnet&quot; package in R, we can perform LASSO regression on our data. # function to calculate mse MSE &lt;- function(y,yhat){ mse = sum((y-yhat)^2) / nrow(y) return(mse) } # data for following analysis datain &lt;- read_csv(&quot;Clean_dataset.csv&quot;, col_names = TRUE, na = c(&quot;NA&quot;)) %&gt;% rename(child_anaemia = &quot;Prevalence of anaemia in children&quot;, pregnant_anameia = &quot;Prevalence of anaemia in pregnant women&quot;, low_weight = &quot;Low birth weight prevalence&quot;, health_expdit = &quot;Health expenditure in GDP&quot;, nurse_midwf = &quot;Nursing and midwifery personnel (per 10 000 population)&quot;, antenatal = &quot;Antenatal care coverage&quot;, breastfed = &quot;Infants breastfed for the first six months&quot; ) set.seed(2019) train_ids = sample(nrow(datain), size = 2/3 * nrow(datain), replace = FALSE) train = datain[train_ids, 3:8] test = datain[-train_ids, 3:8] #---------------------------------------------------------# # LASSO with glmnet # #---------------------------------------------------------# lasso = glmnet(y = as.matrix(train[, 1]), x = as.matrix(train[, 2:6]), alpha=1, standardize=T, family='gaussian') lasso plot(lasso, xvar = &quot;lambda&quot;, label = TRUE) lasso_cv &lt;- cv.glmnet(y = as.matrix(train[,1]), x = as.matrix(train[, 2:6]), alpha=1, standardize=T, family='gaussian') plot(lasso_cv) # check coefficient coef1 &lt;- coef(lasso, s = lasso_cv$lambda.1se) coef2 &lt;- coef(lasso, s = lasso_cv$lambda.min) lasso_est &lt;-predict(lasso, newx=as.matrix(train[,2:6]), s=lasso_cv$lambda.min) lasso_mse &lt;- MSE(train['child_anaemia'], lasso_est) lasso_mse # get the best lambda lasso_cv$lambda.min Fig.2 - Change of MSE with different lambda Fig.3 - Variable exclusion process of LASSO From Figure 2, we can see that when log(λ\\lambdaλ) lies between -1.4 to 0, the model can have the smallest mean squared error. Besides, based on Figure 3, we can see the process of the variables being removed regarding to different log(λ\\lambdaλ). Furthermore, we find the best λ\\lambdaλ = 0.255 of our model by cross-validation method. The optimal output is listed in the table below. Variable coefficient Intercept 8.902 Anaemia in pregnant 1.112 Low birth weight 0 Health expenditure in GDP 0.680 Nursing and midwifery personnel -0.081 Antenatal care -0.164 In the optimal model, the variable &quot;Low birth weight&quot; is removed out. The model's mean squared error is 69.345. &quot;Nursing and midwifery personnel&quot; and &quot;Antenatal care &quot; are negatively related to the occurrence of anaemia in children which is consistent to our expectation. The more health care personnel and antenatal care that pregnant women have, the less likely anaemia in children would happen. However, in this model , the positive relationship between &quot;Health expenditure in GDP&quot; and the occurrence of anaemia in children is not expected. Compare with Linear Regression By using lm() function in R we can easily get the linear regression: #---------------------------------------------------------# # Linear Regression # #---------------------------------------------------------# lm_model &lt;- lm(formula = child_anaemia ~ ., data = train) summary(lm_model) # Predict and estimate the mse on test dataset lm_est &lt;- predict(lm_model, test1) lm_mse &lt;- MSE(test1['child_anaemia'], lm_est) cat(&quot;MSE of final linear model:&quot;, lm_mse) # MSE of final linear model: 61.5218 *Fig.4 - Linear Regression Results * From Figure 4, we can see that not all the variables are significant. In terms of the explanatory power, linear regression and LASSO have a similar R-square value. In terms of prediction accuracy, linear regression has the better accuracy of this dataset. Linear Regression LASSO R-square on training data 0.8216 0.8229 MSE on test data 61.5218 69.345 Conclusion In our case, the linear regression gives better prediction than LASSO regression. The reason could be that the number of independent variables is not big enough. It is not uncommon that a simple regression may give better result than &quot;fancier&quot; models. Likewise, sometimes a logistic regression could perform better than Neural Network. The performance largely depends on the data. Therefore it is useful to make comparison between different models given a dataset. Thanks for reading! ","link":"https://BolinWu-Gridea.github.io/post/2021-03-09-LASSO/"},{"title":"test","content":"test ","link":"https://BolinWu-Gridea.github.io/post/test/"},{"title":"Classification of Baseball Player Technical Statistics by Decision Tree and Random Forest","content":"Introduction Tree-based methods are conceptually easy to comprehend and they render advantages like easy visualization and data-preprocessing. It is a powerful tool for classification. In this post I will introduce how to classify baseball player technical statistics by Decision Tree and Random Forest from algorithm coding to package usage. Content: Showing the algorithm of Decision Tree by R, which including tree splitting, tree grwoing, bagging, prediction, etc. Runing Random Forest and XGBoost with the help of randomForest package Reference: The Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie et al. Mans Magnusson (2020). uuml: R Content for the Introduction to Machine Learning. Course at Uppsala University. R package version 0.2.0. Decision Tree Data Pre-processing First, we need to lead the dataset Hitters provided in the refereced uuml package. This dataset consists of the salary of many baseball players and their relevant technical statistics. Here we assume that we only care about three columns:&quot;Years&quot;, &quot;Hits&quot; and &quot;Salary&quot;. The missing values are imputed by listwise deletion. And we set the first 30 ovservations as test set and the rest as training set. # install relevant packages # remotes::install_github(&quot;MansMeg/IntroML&quot;,subdir = &quot;rpackage&quot;) # install.packages(&quot;randomForest&quot;) # install.packages(&quot;xgboost&quot;) # loead the packages library(xgboost) library(randomForest) # load the data library(uuml) data(&quot;Hitters&quot;) # In the task we only care about three columns:&quot;Years&quot;, &quot;Hits&quot; and &quot;Salary&quot; # and we need to excluede the NA values # So we need to pre-process the data Hitters = Hitters[,c(&quot;Years&quot;, &quot;Hits&quot;,&quot;Salary&quot;)] # get rid of NA Hitters &lt;- Hitters[complete.cases(Hitters),] # set aside test set and training set X_test &lt;- Hitters[1:30, c(&quot;Years&quot;, &quot;Hits&quot;)] y_test &lt;- Hitters[1:30, c(&quot;Salary&quot;)] X_train &lt;- Hitters[31:nrow(Hitters), c(&quot;Years&quot;, &quot;Hits&quot;)] y_train &lt;- Hitters[31:nrow(Hitters), c(&quot;Salary&quot;)] Tree Splitting Now let us see how to do the first split. Please note that here does not involve tree growing. Concept The alforithm we will use is from the referenced book The Elements of Statistical Learning: Data Mining, Inference, and Prediction (ESL), P307. We are seeking to splitting variable j and split point s that meet: minj,s[minc1∑xi∈R1(j,s)(yi−c1)2+minc2∑xi∈R2(j,s)(yi−c2)2]\\begin{aligned} min_{j,s} [min_{c1} \\sum_{x_{i}\\in R_{1}(j,s)}(y_{i} - c_{1})^{2} + min_{c_{2}} \\sum_{x_{i} \\in R_{2}(j,s)}(y_{i} - c_{2})^{2} ] \\end{aligned} minj,s​[minc1​xi​∈R1​(j,s)∑​(yi​−c1​)2+minc2​​xi​∈R2​(j,s)∑​(yi​−c2​)2]​ Where the inner minimization with regard to j and s is solved by : c1^=ave(yi∣xi∈R1(j,s))c2^=ave(yi∣xi∈R2(j,s))\\begin{aligned} \\hat{c_{1}} &amp;= ave (y_{i}| x_{i} \\in R_{1} (j,s)) \\\\ \\hat{c_{2}} &amp;= ave (y_{i}| x_{i} \\in R_{2} (j,s)) \\end{aligned} c1​^​c2​^​​=ave(yi​∣xi​∈R1​(j,s))=ave(yi​∣xi​∈R2​(j,s))​ By first glance, you may get confused by what do those equations mean. Let us use a part of the data to make an illustration: # use a size of 20 X_check &lt;- Hitters[31:50, c(&quot;Years&quot;, &quot;Hits&quot;)] y_check &lt;- Hitters[31:50, c(&quot;Salary&quot;)] head(X_check) ## Years Hits ## -Bob Melvin 2 60 ## -BillyJo Robidoux 2 41 ## -Bill Schroeder 4 46 ## -Chris Bando 6 68 ## -Chris Brown 3 132 ## -Carmen Castillo 5 57 Essentially, what they do can be explained by the following three steps: Let j grind over all the variables of the dataset, which in our case is 2 variables Years and Hits. Let s grind over all the values of jth variable. For example, given the sample data above when j = 1 (Years), s will grind from year = 2 (Bob Melvin) to the year of last player. Allocate each observation according to the given j and s into two groups. And then calculate the mean value of each group, $$\\hat{c_{1}}$$ and $$\\hat{c_{2}}$$. Get the within group scatters by using the function in the min() of the first equation above. Return the j and s that give the smallest within group scatter. This process can be also called greedy method, because we are grinding all the possible values and return the most ideal result. Code To illustrate with R code, we will implement a function that takes data set X, the label y, and a minimal leaf size l. This function will give four outputs: The region that each observation belongs to (R1 and R2), splitting variable j and splitting point s. tree_split = function(X,y,l){ # store the split point S = matrix(NA,nrow = nrow(X), ncol = ncol(X)) # store the sum of square SS = matrix(NA,nrow = nrow(X), ncol = ncol(X)) for (j in 1:ncol(X)) { for (k in 1:nrow(X)) { # use the data point as split point s = X[k,j] # get the size in each leaf R1_size = length( which(X[,j] &lt; s) ) R2_size = length( which(X[,j] &gt;= s) ) # proceed if the size of leaf is bigger than the minimum l if (R1_size &gt;= l &amp; R2_size &gt;=l) { # 2.1.3 c1 = mean( y[which(X[,j] &lt; s)] ) # 2.1.4 c2 = mean( y[which(X[,j] &gt;= s)] ) # 2.1.5 SS[k,j] = sum( (y[which(X[,j] &lt; s)] - c1)^2 ) + sum((y[which(X[,j] &gt;= s)] - c2)^2) } else{ # if the leaf is smaller than the minimum, then set to inf SS[k,j] = Inf } S[k,j] = s } } # find the index of Matix with smallest value j = which(SS == min(SS), arr.ind = TRUE)[1,2]; s = X[which(SS == min(SS), arr.ind = TRUE)[1,1],j]; R1 = which(X[,j] &lt; s); R2 = which(X[,j] &gt;= s) return(list(j = j, s = s, R1 = R1, R2 = R2 , SS = min(SS) ) ) } Then check with the sample data, assuming the minimal leaf size to be 5: tree_split(X_check, y_check, l = 5) $j col 1 $s [1] 6 $R1 [1] 1 2 3 5 6 9 13 16 18 19 $R2 [1] 4 7 8 10 11 12 14 15 17 20 $SS [1] 1346633 The results seem to be reasonable. What about the first split for the whole training data? tree_split(X_train , y_train , l = 5 ) $j col 1 $s [1] 5 $R1 [1] 1 2 3 5 9 13 16 18 19 21 22 [12] 28 30 36 38 40 41 42 45 51 55 57 [23] 73 75 78 79 80 88 89 94 96 99 100 [34] 102 104 107 113 114 118 119 121 128 130 133 [45] 134 138 139 141 142 143 145 146 147 149 151 [56] 152 155 157 167 178 180 183 184 185 187 191 [67] 192 193 194 197 198 199 204 206 210 211 216 [78] 220 222 227 228 $R2 [1] 4 6 7 8 10 11 12 14 15 17 20 [12] 23 24 25 26 27 29 31 32 33 34 35 [23] 37 39 43 44 46 47 48 49 50 52 53 [34] 54 56 58 59 60 61 62 63 64 65 66 [45] 67 68 69 70 71 72 74 76 77 81 82 [56] 83 84 85 86 87 90 91 92 93 95 97 [67] 98 101 103 105 106 108 109 110 111 112 115 [78] 116 117 120 122 123 124 125 126 127 129 131 [89] 132 135 136 137 140 144 148 150 153 154 156 [100] 158 159 160 161 162 163 164 165 166 168 169 [111] 170 171 172 173 174 175 176 177 179 181 182 [122] 186 188 189 190 195 196 200 201 202 203 205 [133] 207 208 209 212 213 214 215 217 218 219 221 [144] 223 224 225 226 229 230 231 232 233 $SS [1] 38464163 The fist split variate is j =1, which is year. The value is 5. If year is smaller than 5, then the observations go to R1, otherwise go to R2. Tree Growing In this part, I will first show the code and then illustrate it. Code Conceptually, tree growing is easy to understand: we looping pre-defined tree splitting until the generated leaves are so small that can not be further splitted (leaf size &lt; 2l). However, it is a bit difficult to implement tree growing by coding. Here I will make a function that takes same X,y, and l. It returns a data frame including j, s, gamma, R1_i and R2_i. Gamma is a metric of within group scatter. The R1_i and R2_i indicates which row of data frame to go next. max_num_leaf = 7 #this does not affact the output, just set the max depth of the tree, the redundant part will show NA grow_tree = function(X,y,l){ # make the matrix to store the data S = matrix(NA,nrow = max_num_leaf, ncol = nrow(X)) j = matrix(NA,nrow = max_num_leaf, ncol = 1) s = matrix(NA,nrow = max_num_leaf, ncol = 1) gamma_m = matrix(NA,nrow = max_num_leaf, ncol = 1) R1_i = rep(NA, length = max_num_leaf); R2_i = rep(NA, length = max_num_leaf) # the initial value S[1,] = c(1 : nrow(X)) M = 1 m = 1 while (m &lt;= M ) { # loop until the size is too small to be splitted if ( length(S[m,][!is.na(S[m,])])&gt;= (2*l) ){ # given a specific m: # get leaf size after split len_col = length((tree_split(X[S[m,],], y[S[m,]], l = l)$R1)) S[M+1,1:len_col ] = tree_split(X[S[m,],], y[S[m,]], l = l)$R1 len_col = length((tree_split(X[S[m,],], y[S[m,]], l = l)$R2)) S[M+2,1:len_col] = tree_split(X[S[m,],], y[S[m,]], l = l)$R2 # get the split variable and point j[m] = tree_split(X[S[m,],], y[S[m,]], l = l)$j s[m] = tree_split(X[S[m,],], y[S[m,]], l = l)$s # move on R1_i[m] = M+1 ; R2_i[m] = M + 2 M = M +2 } else { # when the size is too small, just return gamma, stop increasing M gamma_m [m]= mean( y[S[m,]], na.rm = T ) } m = m + 1 } return(data.frame( j = j, s = s , R1_i=R1_i, R2_i = R2_i, gamma = gamma_m )) } Explanation Here my code may seem a bit too much. I believe that different people will have different approaches to build the algorithm and if you who are reading this post have a neater way please let me know, thank you 😃 Instead of explaining the my code line by line, I would like to explain the general concept with the help of my scrach. Sorry if it is a bit ugly. An ugly scratch The key is to use m (green) and M (yellow). The m denotes the index of splitted leaves, the M denotes the total number of leaves given an iteration. Therefore, as long as when the size of leaf m is bigger than 2l, m increases by 1 step while M goes by 2 steps per iteration (since there are two new leaves after each split). It is kind of like m is chasing M, and M stops when leaves stop growing and m stops when it catches M. Every time when m goes one step, it activates tree splitting function and gathers useful infomation. Let us try out with the sample data: tr = grow_tree(X_check,y_check,l = 5) tr ## j s R1_i R2_i gamma ## 1 6 2 3 NA ## 1 4 4 5 NA ## 2 102 6 7 NA ## NA NA NA NA 317 ## NA NA NA NA 496 ## NA NA NA NA 274 ## NA NA NA NA 539 This data frame can be regarded as a &quot;map&quot;. For example, for observation Bob Melvin; year = 2 hits = 60, firstly since year&lt;6, it follows R2_i = 3, going to 3rd row of the data frame. Secondly, since hits&lt;102, it follows R2_i = 7, going to the 7th row. Thirdly, since there are only NA for indicating next step, the 7th row is its destination. Prediction Finally, we are at an exciting part, predicing a new observation given our pre-trained decision tree! The basic idea of prediction is following the output dataframe of the tr. As is mentioned above, R1_i and R2_i indicates the row of the dataframe to go next like a map. It is stopped until the row shows up NA for the first 4 columns. This function is to predict the classification gamma of new observations. predict_with_tree = function(newdata, tree){ pred = matrix(NA,nrow =1 , ncol = ncol(newdata)) for (i in 1: nrow(newdata)) { # start with m =1, the first row m =1 ; s = tree[m,2] ; j = tree[m,1] while (!is.na(tree[m,1])) { if (newdata[i,j] &lt;s) { # follow R1_i m = tree[m,3];s = tree[m,2] ; j = tree[m,1] }else{# follow R2_i m = tree[m,4];s = tree[m,2] ; j = tree[m,1] } pred[i] = tree[m,5] } } return(pred) } Let us pre: X_new &lt;- Hitters[51:52, c(&quot;Years&quot;, &quot;Hits&quot;)] y_new &lt;- Hitters[51:52, c(&quot;Salary&quot;)] predict_with_tree(newdata = X_new, tree = tr) ## [,1] [,2] ## [1,] 317 496 The gamma of first observation is 317 and the second is 396. What is the mean square error on the test set for a tree trained on the whole training data? # set a large maximum tree depth max_num_leaf = 50 # since we have more observations than the check data # set the minimum leaf size = 10 tr = grow_tree(X_train,y_train,l = 10) pred_tr = predict_with_tree(newdata = X_test, tree = tr) MSE = mean((pred_tr - y_test) ^2 ) cat(&quot;MSE =&quot;,MSE) ## MSE = 78395.21 Bagging Concept The basic idea of bagged tree regression is that we draw with replacement a random sample of N units from the original sample and fit a prediction, then we repeat it B times. In the end we weigh together the B predictions and derive the final prediction. The picture on P285, ESL tells us that as the number of Bootstrap samples goes greater, the test error goes smaller then it tends to be a constant which is smaller than the original tree. Figure from P285, ESL Code bagged_tree = function(Xtrain, Ytrain, l, B,Xtest){ sizeN = nrow(Xtrain) # store the predictions bagged_pred = matrix(NA, nrow = B, ncol = nrow(Xtest)) for (i in (1:B)) { # bootstrap sample random_draw = sample( c(1:sizeN ), size = sizeN ,replace = T ) bagged_tr = grow_tree(Xtrain[random_draw,],Ytrain[random_draw],l) bagged_pred_tr = predict_with_tree(newdata = Xtest, tree = bagged_tr) bagged_pred[i,] = bagged_pred_tr i = i + 1 } # the final prediction is the mean of the B trees prediction return(colMeans(bagged_pred) ) } Let us see if the B goes bigger, will RMSE goes smaller: set.seed(100) cat(&quot;B = 10, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=10,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot;, &quot;B = 20, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=20,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot;, &quot;B = 30, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=30,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot; ) ## B = 10, bagged tree RMSE = 312.8866 ## B = 20, bagged tree RMSE = 309.4088 ## B = 30, bagged tree RMSE = 295.2994 The RMSE goes smaller indeed. Random Forest and Boosting Concept The idea of random forest is very similar to bagged tree model. There is only one difference that in bagged tree model, all the features in the bootstrap samples are used. But in the random forest only random subset (without replacement) of features are chosen. The random forest is supposed to give a better performance if the trees are highly correlated. The intuition of boosting is that the training of latter tree is learning from the misclassification of the previous tree. So that the next trained tree is better than the previous tree. For this part, we just need to fit the data into randomForest() function. ntree controls the number of bootstrap samples. To make it comparable, I also set ntree to be 10 which is the same as the previous bagged tree regression. Code train_df = Hitters[31:nrow(Hitters),] rf_mod = randomForest(Salary~ . , data = train_df, ntree = 10) rf_mod ## ## Call: ## randomForest(formula = Salary ~ ., data = train_df, ntree = 10) ## Type of random forest: regression ## Number of trees: 10 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 182725.5 ## % Var explained: 15.89 The variable that are used is only 1. I suppose the reason is that there are only 2 variables in the X train data. According to the rule of thumb, the number of splitted variable is K/3 for regression model. In our case it is 2/3, which is rounded to be 1. Now we can feed the randomForest function with xtest and ytest so that we can get the MSE of the test set. set.seed(100) rf_mod = randomForest(Salary~ . , data = train_df,xtest = X_test ,ytest = y_test, ntree = 10) rf_mod ## Call: ## randomForest(formula = Salary ~ ., data = train_df, xtest = X_test, ytest = y_test, ntree = 10) ## Type of random forest: regression ## Number of trees: 10 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 193201.2 ## % Var explained: 11.06 ## Test set MSE: 67380.79 ## % Var explained: 24.66 cat(&quot;random forest RMSE of test set =&quot;, sqrt(67380.79)) ## random forest RMSE of test set = 259.5781 After reading the XG boosting documentation, I assume the parameter nrounds control the number of the tree therefore I set it to be 10 to make it comparable with the previous results. xgb = xgboost(data = data.matrix(X_train), label = y_train, max.depth =5, eta = 1, nthread = 2, nrounds =10 ) And the RMSE of the predictions can be calculated as follows: y_pred &lt;- predict(xgb, data.matrix(X_test)) cat(&quot;RMSE of xgboost = &quot;, sqrt(mean((y_pred - y_test)^2)) ) ## RMSE of xgboost = 285.6745 The RMSE is bigger than the random forest model. It could be the reason that the sample size is not big enough or I did the tune the prameters in the function well. However, it is better than the bagged tree model. Ending Challenges Conceptually, tree based methods are not difficult to understand. However, depending on your background, it might be difficult to implement them by plain coding. For example when I was coding the tree growing algorithm, I was struggled with grasping m and M. And also it is easy to code the tree growing process when depth = 2 or 3 but it could be hard to generalize it. It requires a good understanding of looping. Nevertheless, the struggling process does help me to understand the algorithm better. I would encourage the reader to get your hand dirty by starting from scratch despite the fact that there are packages which can make it work easily. Tips The graphic illustration of how tree-based methods partitioning feature into a set of rectangles is pretty good. Please check out on ESL P306. ","link":"https://BolinWu-Gridea.github.io/post/2020-02-18-DecisionTree/"}]}