{"posts":[{"title":"An Overview of Causal Inference (Part 1: Causal Effects and Confounding)","content":"Introduction Causal inference has been a heated field in statistics. It has great application for observational data. In this post I will shares some key concepts of causal inference: The confusion over causal inference The important causal assumptions The concept of causal effects Confounding and Directed Acyclic Graphs Main reference: A Crash Course in Causality: Inferring Causal Effects from Observational Data from Coursera. Confusion over causality Spurious correlation Causally unrelated variables might happen to be highly correlated with each other over some period of time. E.g. Divorce rate in Marine correlates with Percapita consumtion of margarine. Anecdotes Example: Bill Smith lived to be 105 years old. He said the secret to his longevity was eating one turnip a day. All we know is that Bill Smith lived to be 105 years old AND he ate one turnip a day. We do not know if eating turnips contributed to his lifespan. We do not know what would happen if other people adopted this habbit. Science reporting Headlines often do not use the forms of the word cause, but do get interpreted causally. Example: Positive link between video games and acedemic performance, study suggests. In reality, a lot of times how skeptically people view headlines depends on what their prior beliefs are. In causality analysis, we want to move away from that to a large degree. Instead, we want to look at the evidence as it is. Some key points: What statistical methods did they use? How was the study designed? What assumptions did they made? Reverse causality Even if there is a causal relationship, sometimes the direction is unclear. Example: Urban green space and exercise. Does green space in urban environments cause people to exercise more? Or the fact that more people come to exercise causes the govenment to build more gree space? How to clear up confusion? The dield of causal inference attempts to do this by proposing: Formal definitions of causal effects. Assumptions necessary to identify causal effects from data. Rules about what variables need to be controlled for. Sensitivity analysis to determine the impact of violations of assumptions on conclusions. Statisticians started working on causal modeling as far back as the 1920s (Wright 1921; Neyman 1923) It became its own area of statistical research since around 1970s. Some highlights: Re-introduction of potential outcomes: Rubin causal model (Rubin 1974). Causal diagrams (Greenland and Robins 1986; Pearl 2000). Propensity scores (Rosenbaum and Rubin 1983). Time-dependent confounding (Robins 1986; Robins 1997). Optimal dynamic treatment strategies (Murphy 2003; Robins 2004). Target learning (vander Laan 2009). As we dive deeper into causal modeling, it will be important to remember: Causal inference requires making some untestable assumptions (reffered to as causal assumptions) Cochran (1972) concludes: &quot;...observational studies are an interesting and challenging field which demands a good quality of humility, since we can claim only to be groping toward the truth.&quot; Treatment, potential outcomes and counterfactuals Here we will introduce some notations that is important for the following post. Suppose we are interested in the causal effect of some treatment A on some outcome Y. treatment example: A = 1 if receive influenza vaccine; A = 0 otherwise. Here is a treatment that takes two values 1 or 0. Outcome example: Y = 1 if develop cardiovascular disease within 2 years; Y = 0 otherwise. What is potential outcomes? You can think of it as the possible outcomes before the study takes place. Notation: $$Y^{a}$$ is the outcome that would be observed if treatment was set to A = a. What about counterfactuals? Counterfactual outcomes are ones that would have been ovserved had the treatment been different. For example: if my treatment wes A = 1, then my counterfactual outcome is $$Y^{0}$$. Hypothetical intervention One important assumption of causal effects of intervention is that the variables can be manipulated. Halland (1986) famously wrote &quot;no causation without manipulation&quot;&quot;. We can imagine that we can manupulate some people get drug A while others get drugs B. However, it is less clear about what a causal effect of an immutable variable would mean e.g. gender, age, race. One way to approch this is to relate these varibales to the variables that we can manupulate. No direct intervention Manipulable intervention Race Name on resume Obesity Bariatric surgery Socioeconomic status Gift of money For the remainder of the post, we will primarily focus on treaments that could be thought of as interventions. Treatments that we can imagine being randomized (manupulated) in a hypothetical trial. The reason that we focus on causal effect of hypothetical interventions is that Their meaning is well defined. They are potentially actionable. What are causal effects? In general: A had a causal effect on Y if $$Y^{1}$$ differs from $$Y^{0}$$. The foundamental problem of causal inference is that we can only observe one potential outcome for each person. However, with certain assumptions, we can estimate pupulation level (average) causal effects. That is, rather than think if the causal effect work for individual, we think of the population as a whole. Therefore we never know the unit level causal effect. Hopeless: What would have happened to me had I not taken ibuprofen? (unit elvel causal effect) Possible: What would the rate of headache remission be if everyone took ibuprofen when they had a headache versus no one did? Average ausal Effect Definition: E(Y1−Y0)E(Y^{1} - Y^{0})E(Y1−Y0). It means the average value of Y if everyone was treated with A = 1 minus the average value of Y if everyone was treated with A = 0, if Y is binary. Please note that this is just an ideal definition because we could never actually observe that in the real world. Conditioning on, VS setting, treatment In general, E(Y1−Y0)≠E(Y∣A=1)−E(Y∣A=0)\\begin{aligned} E(Y^{1} - Y^{0}) \\neq E(Y|A = 1) - E(Y|A = 0) \\end{aligned} E(Y1−Y0)​=E(Y∣A=1)−E(Y∣A=0)​ The reason is that $$E(Y^{1}$$ is the mean of Y if the whole population was treated with A = 1; while $$E(Y|A = 1)$$ is mean of Y among people with A = 1. Technically, $$E(Y|A = 1) - E(Y|A = 0)$$ is not a causal effect because it is comparing two different populations of people. Other causal effects Other causal effects that we may be interested in are: E(Y^{1} / Y^{0})$$ : causal relative risk Challenge How do we use observed data to link observed outcomes to potential outcomes? What assumptions are necessay to estimate causal effects from observed data? Causal assumptions Identifiability of causal effects requires making some untestable assumptions. These are generally called causal assumptions. The most common are : Stable Unit Treatment Value Assumption (SUTVA) Consistency Ignorability Positivity They are all about the observed data: Y, A and a set of pre-treatment covariates X. SUTVA It involves two assumptions: No interference: Unites do not interfere with each other Treatment assigment of one unit does not affect that outcome of another unit. &quot;Spoillover&quot; or &quot;contagion&quot; are also terms for interference. One version of treatment The potential outcomes can effectively linked to the observed data. SUTVA allows us to write potential outcome for the ith person in terms of only that person's treatments. Consistency assumption The potential outcome under treatment A = a is equal to the observed outcome if the actual treatment received is A = a. Ignorability assumption Given pre-treatment covariates X, treatment assignment is independent from the potential outcomes. Y0,Y1∐A∣XY^{0}, Y^{1} \\coprod A|X Y0,Y1∐A∣X Essentially it means that the treatment A is randomly assigned regardless of X. Positivity assumtion It refers to that everybody has a positive opportunity to receive either treatment. P(A=a∣X=x)&gt;0,for all a and xP(A = a| X = x) &gt;0 , \\text{for all a and x} P(A=a∣X=x)&gt;0,for all a and x If treatment was deterministic for some values of X, then we would have no ovserved values of Y for one of the treatment groups for thoses values of X. Confounding and Directed Acyclic Graphs (DAGs) Confounding control Confounders are often defined as variables that affect treatment and affect the outcome. We are interested in identifying a set of variables X that will make the ignorability assumption hold. Then we want to use statistical methods, which will be covered later in the course, to control these variables and estimated causal effects. Causal graph Graphs (causal graphs or directed acyclic graphs) are considered useful for causal inference. The functions of causal graphs are: Helpful for identifying which variables to control for Make assumptions eplicit. Here I would not explain all the details about DAGs, more information about compatibility between DAGs and distributions can be found here. Instead, I would like to note down some interesting facts when learning DAGs path and associations. For a fork path $$A &lt;-E-&gt; B$$, A and B are dependent because the information from E flows to both A and B. For colliders $$A-&gt;G&lt;-B$$, A and B are independent. However, if we control forff G, then A and B are dependent. Ending Now we have an overview of the causal effects and we know that DAGs is an important method to identify the variables need to be controlled in order to achieve ignorability assumption. Next we will proceed to see how to control the counfounders. The two of the general approaches are matching and inverse probability of treatment weighting. ","link":"https://BolinWu-Gridea.github.io/post/2021-04-20-CausalityIntroductionP1/"},{"title":"test","content":"test ","link":"https://BolinWu-Gridea.github.io/post/test/"},{"title":"Classification of Baseball Player Technical Statistics by Decision Tree and Random Forest","content":"Introduction Tree-based methods are conceptually easy to comprehend and they render advantages like easy visualization and data-preprocessing. It is a powerful tool for classification. In this post I will introduce how to classify baseball player technical statistics by Decision Tree and Random Forest from algorithm coding to package usage. Content: Showing the algorithm of Decision Tree by R, which including tree splitting, tree grwoing, bagging, prediction, etc. Runing Random Forest and XGBoost with the help of randomForest package Reference: The Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie et al. Mans Magnusson (2020). uuml: R Content for the Introduction to Machine Learning. Course at Uppsala University. R package version 0.2.0. Decision Tree Data Pre-processing First, we need to lead the dataset Hitters provided in the refereced uuml package. This dataset consists of the salary of many baseball players and their relevant technical statistics. Here we assume that we only care about three columns:&quot;Years&quot;, &quot;Hits&quot; and &quot;Salary&quot;. The missing values are imputed by listwise deletion. And we set the first 30 ovservations as test set and the rest as training set. # install relevant packages # remotes::install_github(&quot;MansMeg/IntroML&quot;,subdir = &quot;rpackage&quot;) # install.packages(&quot;randomForest&quot;) # install.packages(&quot;xgboost&quot;) # loead the packages library(xgboost) library(randomForest) # load the data library(uuml) data(&quot;Hitters&quot;) # In the task we only care about three columns:&quot;Years&quot;, &quot;Hits&quot; and &quot;Salary&quot; # and we need to excluede the NA values # So we need to pre-process the data Hitters = Hitters[,c(&quot;Years&quot;, &quot;Hits&quot;,&quot;Salary&quot;)] # get rid of NA Hitters &lt;- Hitters[complete.cases(Hitters),] # set aside test set and training set X_test &lt;- Hitters[1:30, c(&quot;Years&quot;, &quot;Hits&quot;)] y_test &lt;- Hitters[1:30, c(&quot;Salary&quot;)] X_train &lt;- Hitters[31:nrow(Hitters), c(&quot;Years&quot;, &quot;Hits&quot;)] y_train &lt;- Hitters[31:nrow(Hitters), c(&quot;Salary&quot;)] Tree Splitting Now let us see how to do the first split. Please note that here does not involve tree growing. Concept The alforithm we will use is from the referenced book The Elements of Statistical Learning: Data Mining, Inference, and Prediction (ESL), P307. We are seeking to splitting variable j and split point s that meet: minj,s[minc1∑xi∈R1(j,s)(yi−c1)2+minc2∑xi∈R2(j,s)(yi−c2)2]\\begin{aligned} min_{j,s} [min_{c1} \\sum_{x_{i}\\in R_{1}(j,s)}(y_{i} - c_{1})^{2} + min_{c_{2}} \\sum_{x_{i} \\in R_{2}(j,s)}(y_{i} - c_{2})^{2} ] \\end{aligned} minj,s​[minc1​xi​∈R1​(j,s)∑​(yi​−c1​)2+minc2​​xi​∈R2​(j,s)∑​(yi​−c2​)2]​ Where the inner minimization with regard to j and s is solved by : c1^=ave(yi∣xi∈R1(j,s))c2^=ave(yi∣xi∈R2(j,s))\\begin{aligned} \\hat{c_{1}} &amp;= ave (y_{i}| x_{i} \\in R_{1} (j,s)) \\\\ \\hat{c_{2}} &amp;= ave (y_{i}| x_{i} \\in R_{2} (j,s)) \\end{aligned} c1​^​c2​^​​=ave(yi​∣xi​∈R1​(j,s))=ave(yi​∣xi​∈R2​(j,s))​ By first glance, you may get confused by what do those equations mean. Let us use a part of the data to make an illustration: # use a size of 20 X_check &lt;- Hitters[31:50, c(&quot;Years&quot;, &quot;Hits&quot;)] y_check &lt;- Hitters[31:50, c(&quot;Salary&quot;)] head(X_check) ## Years Hits ## -Bob Melvin 2 60 ## -BillyJo Robidoux 2 41 ## -Bill Schroeder 4 46 ## -Chris Bando 6 68 ## -Chris Brown 3 132 ## -Carmen Castillo 5 57 Essentially, what they do can be explained by the following three steps: Let j grind over all the variables of the dataset, which in our case is 2 variables Years and Hits. Let s grind over all the values of jth variable. For example, given the sample data above when j = 1 (Years), s will grind from year = 2 (Bob Melvin) to the year of last player. Allocate each observation according to the given j and s into two groups. And then calculate the mean value of each group, $$\\hat{c_{1}}$$ and $$\\hat{c_{2}}$$. Get the within group scatters by using the function in the min() of the first equation above. Return the j and s that give the smallest within group scatter. This process can be also called greedy method, because we are grinding all the possible values and return the most ideal result. Code To illustrate with R code, we will implement a function that takes data set X, the label y, and a minimal leaf size l. This function will give four outputs: The region that each observation belongs to (R1 and R2), splitting variable j and splitting point s. tree_split = function(X,y,l){ # store the split point S = matrix(NA,nrow = nrow(X), ncol = ncol(X)) # store the sum of square SS = matrix(NA,nrow = nrow(X), ncol = ncol(X)) for (j in 1:ncol(X)) { for (k in 1:nrow(X)) { # use the data point as split point s = X[k,j] # get the size in each leaf R1_size = length( which(X[,j] &lt; s) ) R2_size = length( which(X[,j] &gt;= s) ) # proceed if the size of leaf is bigger than the minimum l if (R1_size &gt;= l &amp; R2_size &gt;=l) { # 2.1.3 c1 = mean( y[which(X[,j] &lt; s)] ) # 2.1.4 c2 = mean( y[which(X[,j] &gt;= s)] ) # 2.1.5 SS[k,j] = sum( (y[which(X[,j] &lt; s)] - c1)^2 ) + sum((y[which(X[,j] &gt;= s)] - c2)^2) } else{ # if the leaf is smaller than the minimum, then set to inf SS[k,j] = Inf } S[k,j] = s } } # find the index of Matix with smallest value j = which(SS == min(SS), arr.ind = TRUE)[1,2]; s = X[which(SS == min(SS), arr.ind = TRUE)[1,1],j]; R1 = which(X[,j] &lt; s); R2 = which(X[,j] &gt;= s) return(list(j = j, s = s, R1 = R1, R2 = R2 , SS = min(SS) ) ) } Then check with the sample data, assuming the minimal leaf size to be 5: tree_split(X_check, y_check, l = 5) $j col 1 $s [1] 6 $R1 [1] 1 2 3 5 6 9 13 16 18 19 $R2 [1] 4 7 8 10 11 12 14 15 17 20 $SS [1] 1346633 The results seem to be reasonable. What about the first split for the whole training data? tree_split(X_train , y_train , l = 5 ) $j col 1 $s [1] 5 $R1 [1] 1 2 3 5 9 13 16 18 19 21 22 [12] 28 30 36 38 40 41 42 45 51 55 57 [23] 73 75 78 79 80 88 89 94 96 99 100 [34] 102 104 107 113 114 118 119 121 128 130 133 [45] 134 138 139 141 142 143 145 146 147 149 151 [56] 152 155 157 167 178 180 183 184 185 187 191 [67] 192 193 194 197 198 199 204 206 210 211 216 [78] 220 222 227 228 $R2 [1] 4 6 7 8 10 11 12 14 15 17 20 [12] 23 24 25 26 27 29 31 32 33 34 35 [23] 37 39 43 44 46 47 48 49 50 52 53 [34] 54 56 58 59 60 61 62 63 64 65 66 [45] 67 68 69 70 71 72 74 76 77 81 82 [56] 83 84 85 86 87 90 91 92 93 95 97 [67] 98 101 103 105 106 108 109 110 111 112 115 [78] 116 117 120 122 123 124 125 126 127 129 131 [89] 132 135 136 137 140 144 148 150 153 154 156 [100] 158 159 160 161 162 163 164 165 166 168 169 [111] 170 171 172 173 174 175 176 177 179 181 182 [122] 186 188 189 190 195 196 200 201 202 203 205 [133] 207 208 209 212 213 214 215 217 218 219 221 [144] 223 224 225 226 229 230 231 232 233 $SS [1] 38464163 The fist split variate is j =1, which is year. The value is 5. If year is smaller than 5, then the observations go to R1, otherwise go to R2. Tree Growing In this part, I will first show the code and then illustrate it. Code Conceptually, tree growing is easy to understand: we looping pre-defined tree splitting until the generated leaves are so small that can not be further splitted (leaf size &lt; 2l). However, it is a bit difficult to implement tree growing by coding. Here I will make a function that takes same X,y, and l. It returns a data frame including j, s, gamma, R1_i and R2_i. Gamma is a metric of within group scatter. The R1_i and R2_i indicates which row of data frame to go next. max_num_leaf = 7 #this does not affact the output, just set the max depth of the tree, the redundant part will show NA grow_tree = function(X,y,l){ # make the matrix to store the data S = matrix(NA,nrow = max_num_leaf, ncol = nrow(X)) j = matrix(NA,nrow = max_num_leaf, ncol = 1) s = matrix(NA,nrow = max_num_leaf, ncol = 1) gamma_m = matrix(NA,nrow = max_num_leaf, ncol = 1) R1_i = rep(NA, length = max_num_leaf); R2_i = rep(NA, length = max_num_leaf) # the initial value S[1,] = c(1 : nrow(X)) M = 1 m = 1 while (m &lt;= M ) { # loop until the size is too small to be splitted if ( length(S[m,][!is.na(S[m,])])&gt;= (2*l) ){ # given a specific m: # get leaf size after split len_col = length((tree_split(X[S[m,],], y[S[m,]], l = l)$R1)) S[M+1,1:len_col ] = tree_split(X[S[m,],], y[S[m,]], l = l)$R1 len_col = length((tree_split(X[S[m,],], y[S[m,]], l = l)$R2)) S[M+2,1:len_col] = tree_split(X[S[m,],], y[S[m,]], l = l)$R2 # get the split variable and point j[m] = tree_split(X[S[m,],], y[S[m,]], l = l)$j s[m] = tree_split(X[S[m,],], y[S[m,]], l = l)$s # move on R1_i[m] = M+1 ; R2_i[m] = M + 2 M = M +2 } else { # when the size is too small, just return gamma, stop increasing M gamma_m [m]= mean( y[S[m,]], na.rm = T ) } m = m + 1 } return(data.frame( j = j, s = s , R1_i=R1_i, R2_i = R2_i, gamma = gamma_m )) } Explanation Here my code may seem a bit too much. I believe that different people will have different approaches to build the algorithm and if you who are reading this post have a neater way please let me know, thank you 😃 Instead of explaining the my code line by line, I would like to explain the general concept with the help of my scrach. Sorry if it is a bit ugly. An ugly scratch The key is to use m (green) and M (yellow). The m denotes the index of splitted leaves, the M denotes the total number of leaves given an iteration. Therefore, as long as when the size of leaf m is bigger than 2l, m increases by 1 step while M goes by 2 steps per iteration (since there are two new leaves after each split). It is kind of like m is chasing M, and M stops when leaves stop growing and m stops when it catches M. Every time when m goes one step, it activates tree splitting function and gathers useful infomation. Let us try out with the sample data: tr = grow_tree(X_check,y_check,l = 5) tr ## j s R1_i R2_i gamma ## 1 6 2 3 NA ## 1 4 4 5 NA ## 2 102 6 7 NA ## NA NA NA NA 317 ## NA NA NA NA 496 ## NA NA NA NA 274 ## NA NA NA NA 539 This data frame can be regarded as a &quot;map&quot;. For example, for observation Bob Melvin; year = 2 hits = 60, firstly since year&lt;6, it follows R2_i = 3, going to 3rd row of the data frame. Secondly, since hits&lt;102, it follows R2_i = 7, going to the 7th row. Thirdly, since there are only NA for indicating next step, the 7th row is its destination. Prediction Finally, we are at an exciting part, predicing a new observation given our pre-trained decision tree! The basic idea of prediction is following the output dataframe of the tr. As is mentioned above, R1_i and R2_i indicates the row of the dataframe to go next like a map. It is stopped until the row shows up NA for the first 4 columns. This function is to predict the classification gamma of new observations. predict_with_tree = function(newdata, tree){ pred = matrix(NA,nrow =1 , ncol = ncol(newdata)) for (i in 1: nrow(newdata)) { # start with m =1, the first row m =1 ; s = tree[m,2] ; j = tree[m,1] while (!is.na(tree[m,1])) { if (newdata[i,j] &lt;s) { # follow R1_i m = tree[m,3];s = tree[m,2] ; j = tree[m,1] }else{# follow R2_i m = tree[m,4];s = tree[m,2] ; j = tree[m,1] } pred[i] = tree[m,5] } } return(pred) } Let us pre: X_new &lt;- Hitters[51:52, c(&quot;Years&quot;, &quot;Hits&quot;)] y_new &lt;- Hitters[51:52, c(&quot;Salary&quot;)] predict_with_tree(newdata = X_new, tree = tr) ## [,1] [,2] ## [1,] 317 496 The gamma of first observation is 317 and the second is 396. What is the mean square error on the test set for a tree trained on the whole training data? # set a large maximum tree depth max_num_leaf = 50 # since we have more observations than the check data # set the minimum leaf size = 10 tr = grow_tree(X_train,y_train,l = 10) pred_tr = predict_with_tree(newdata = X_test, tree = tr) MSE = mean((pred_tr - y_test) ^2 ) cat(&quot;MSE =&quot;,MSE) ## MSE = 78395.21 Bagging Concept The basic idea of bagged tree regression is that we draw with replacement a random sample of N units from the original sample and fit a prediction, then we repeat it B times. In the end we weigh together the B predictions and derive the final prediction. The picture on P285, ESL tells us that as the number of Bootstrap samples goes greater, the test error goes smaller then it tends to be a constant which is smaller than the original tree. Figure from P285, ESL Code bagged_tree = function(Xtrain, Ytrain, l, B,Xtest){ sizeN = nrow(Xtrain) # store the predictions bagged_pred = matrix(NA, nrow = B, ncol = nrow(Xtest)) for (i in (1:B)) { # bootstrap sample random_draw = sample( c(1:sizeN ), size = sizeN ,replace = T ) bagged_tr = grow_tree(Xtrain[random_draw,],Ytrain[random_draw],l) bagged_pred_tr = predict_with_tree(newdata = Xtest, tree = bagged_tr) bagged_pred[i,] = bagged_pred_tr i = i + 1 } # the final prediction is the mean of the B trees prediction return(colMeans(bagged_pred) ) } Let us see if the B goes bigger, will RMSE goes smaller: set.seed(100) cat(&quot;B = 10, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=10,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot;, &quot;B = 20, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=20,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot;, &quot;B = 30, bagged tree RMSE = &quot;,sqrt (mean(( bagged_tree(X_train,y_train,l = 10, B=30,Xtest = X_test) - y_test) ^2 ) ), &quot;\\n&quot; ) ## B = 10, bagged tree RMSE = 312.8866 ## B = 20, bagged tree RMSE = 309.4088 ## B = 30, bagged tree RMSE = 295.2994 The RMSE goes smaller indeed. Random Forest and Boosting Concept The idea of random forest is very similar to bagged tree model. There is only one difference that in bagged tree model, all the features in the bootstrap samples are used. But in the random forest only random subset (without replacement) of features are chosen. The random forest is supposed to give a better performance if the trees are highly correlated. The intuition of boosting is that the training of latter tree is learning from the misclassification of the previous tree. So that the next trained tree is better than the previous tree. For this part, we just need to fit the data into randomForest() function. ntree controls the number of bootstrap samples. To make it comparable, I also set ntree to be 10 which is the same as the previous bagged tree regression. Code train_df = Hitters[31:nrow(Hitters),] rf_mod = randomForest(Salary~ . , data = train_df, ntree = 10) rf_mod ## ## Call: ## randomForest(formula = Salary ~ ., data = train_df, ntree = 10) ## Type of random forest: regression ## Number of trees: 10 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 182725.5 ## % Var explained: 15.89 The variable that are used is only 1. I suppose the reason is that there are only 2 variables in the X train data. According to the rule of thumb, the number of splitted variable is K/3 for regression model. In our case it is 2/3, which is rounded to be 1. Now we can feed the randomForest function with xtest and ytest so that we can get the MSE of the test set. set.seed(100) rf_mod = randomForest(Salary~ . , data = train_df,xtest = X_test ,ytest = y_test, ntree = 10) rf_mod ## Call: ## randomForest(formula = Salary ~ ., data = train_df, xtest = X_test, ytest = y_test, ntree = 10) ## Type of random forest: regression ## Number of trees: 10 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 193201.2 ## % Var explained: 11.06 ## Test set MSE: 67380.79 ## % Var explained: 24.66 cat(&quot;random forest RMSE of test set =&quot;, sqrt(67380.79)) ## random forest RMSE of test set = 259.5781 After reading the XG boosting documentation, I assume the parameter nrounds control the number of the tree therefore I set it to be 10 to make it comparable with the previous results. xgb = xgboost(data = data.matrix(X_train), label = y_train, max.depth =5, eta = 1, nthread = 2, nrounds =10 ) And the RMSE of the predictions can be calculated as follows: y_pred &lt;- predict(xgb, data.matrix(X_test)) cat(&quot;RMSE of xgboost = &quot;, sqrt(mean((y_pred - y_test)^2)) ) ## RMSE of xgboost = 285.6745 The RMSE is bigger than the random forest model. It could be the reason that the sample size is not big enough or I did the tune the prameters in the function well. However, it is better than the bagged tree model. Ending Challenges Conceptually, tree based methods are not difficult to understand. However, depending on your background, it might be difficult to implement them by plain coding. For example when I was coding the tree growing algorithm, I was struggled with grasping m and M. And also it is easy to code the tree growing process when depth = 2 or 3 but it could be hard to generalize it. It requires a good understanding of looping. Nevertheless, the struggling process does help me to understand the algorithm better. I would encourage the reader to get your hand dirty by starting from scratch despite the fact that there are packages which can make it work easily. Tips The graphic illustration of how tree-based methods partitioning feature into a set of rectangles is pretty good. Please check out on ESL P306. ","link":"https://BolinWu-Gridea.github.io/post/2020-02-18-DecisionTree/"}]}