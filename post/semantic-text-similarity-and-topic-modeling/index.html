<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Semantic Text Similarity and Topic Modeling  - Bolin Wu</title>
<link rel="shortcut icon" href="https://BolinWu-Gridea.github.io/favicon.ico">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css">
<link rel="stylesheet" href="https://BolinWu-Gridea.github.io/media/css/tailwind.css">
<link rel="stylesheet" href="https://BolinWu-Gridea.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Semantic Text Similarity and Topic Modeling  - Bolin Wu - Atom Feed" href="https://BolinWu-Gridea.github.io/atom.xml">

    

  <meta name="description" content="
Topic modeling is a useful tool for people to grasp a general picture of a long text document. Compared with LSTM or RN..." />
  <meta property="og:title" content="Semantic Text Similarity and Topic Modeling  - Bolin Wu">
  <meta property="og:description" content="
Topic modeling is a useful tool for people to grasp a general picture of a long text document. Compared with LSTM or RN..." />
  <meta property="og:type" content="articles">
  <meta property="og:url" content="https://BolinWu-Gridea.github.io/post/semantic-text-similarity-and-topic-modeling/" />
  <meta property="og:image" content="https://BolinWu-Gridea.github.io/post-images/semantic-text-similarity-and-topic-modeling.jpg">
  <meta property="og:image:height" content="630">
  <meta property="og:image:width" content="1200">
  <meta name="twitter:title" content="Semantic Text Similarity and Topic Modeling  - Bolin Wu">
  <meta name="twitter:description" content="
Topic modeling is a useful tool for people to grasp a general picture of a long text document. Compared with LSTM or RN...">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="canonical" href="https://BolinWu-Gridea.github.io/post/semantic-text-similarity-and-topic-modeling/">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
 
  
    <link rel="stylesheet" href="https://BolinWu-Gridea.github.io/media/css/prism-atom-dark.css">
  

  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
  
</head>

<body>
  <div class="antialiased flex flex-col min-h-screen" id="app">
    <a href="https://BolinWu-Gridea.github.io" class="fixed top-0 left-0 mt-4 bg-black text-white dark:text-gray-700 dark:bg-yellow-50 dark:hover:bg-black dark:hover:text-white inline-flex p-2 pl-8 hover:text-gray-700 hover:bg-yellow-50 font-bold z-10 transition-fast animated fadeInLeft">
      Bolin Wu
    </a>
    <div class="max-w-4xl w-full mx-auto">
      <div class="shadow-box bg-white dark:bg-gray-600 rounded-lg pt-32 md:pt-64 px-4 md:px-8 pb-8 animated fadeIn mb-8">
        <h1 class="text-5xl font-semibold leading-normal pb-8 mb-8 border-b-8 border-gray-700">
          Semantic Text Similarity and Topic Modeling 
        </h1>
        
          <img src="https://BolinWu-Gridea.github.io/post-images/semantic-text-similarity-and-topic-modeling.jpg" alt="Semantic Text Similarity and Topic Modeling " class="block w-full mb-8">
        
        <div class="mb-8 flex flex-wrap">
          <div class="text-gray-400 text-sm mr-4">2021-07-28 · 19 min read</div>
          
            <a href="https://BolinWu-Gridea.github.io/tag/zOshREnVK/" class="text-gray-700 text-sm border-b-2 border-dotted border-gray-200 hover:border-gray-600 transition-all duration-100 inline-flex mr-2">
              <i class="ri-hashtag"></i>
              Natural Language Process
            </a>
          
            <a href="https://BolinWu-Gridea.github.io/tag/GWOcOUTN0/" class="text-gray-700 text-sm border-b-2 border-dotted border-gray-200 hover:border-gray-600 transition-all duration-100 inline-flex mr-2">
              <i class="ri-hashtag"></i>
              Python
            </a>
          
        </div>
        <div class="markdown mb-8" v-pre>
          <!-- more -->
<p>Topic modeling is a useful tool for people to grasp a general picture of a long text document. Compared with LSTM or RNN, topic model is more or less for observatory purpose rather than prediction. In this post I will share the measure of similarity among words, the concept of topic modeling and its application in Python.</p>
<!-- more -->
<h2 id="semantic-text-similarity">Semantic text similarity</h2>
<p>If we have a text document or a text passage and a sentence. Based on the information in the text passage, we need to say whether the sentence is correct or it derives its meaning from there or not. This is a typical task of semantic similarity. One of the useful resources for semantic similarity analysis is <strong>WordNet</strong>.</p>
<h3 id="wordnet">WordNet</h3>
<p>WordNet is a semantic dictionary of words, interlinked by semantic relations. It is mostly developped in English but not it also available for quite a few other languages. It includes rich linguistic information like part of speech, sense of a word, derivationally related forms, etc. WordNet organizes information in a hierarchy. An example could be like <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2FA-New-Measure-of-Word-Semantic-Similarity-Based-on-Qin-Lu%2Fd3a0c2a70d3d39a5b41b4ef8c4890a481307259f%2Ffigure%2F0&amp;psig=AOvVaw3_5C9ml5D5en1Tn0pVOZ3d&amp;ust=1627380418249000&amp;source=images&amp;cd=vfe&amp;ved=0CAsQjRxqFwoTCLDaktS-gPICFQAAAAAdAAAAABAJ">this</a>.One measure of using such hierarchy is by <strong>path similarity</strong>. It is calculated by finding the shortest path between the two concepts. One way to find it is to count how many steps is needed to take to get from one word to the other in the hierarchy. Similarity measure inversely related to path distance.</p>
<p>In Python, WordNet can be imported through NLTK.</p>
<pre><code>import nltk
from nltk.corpus import wordnet as wn

# find appropriate meaning of words
bus = wn.synset('bus.n.01‘)
train = wn.synset('train.n.01’)
# '.n' means finding the noun form
# '.01' means finding the first definition
</code></pre>
<p>Once we find the proper meaning of the word, we could find the path similarity as follows.</p>
<pre><code>bus.path_similarity(train)
</code></pre>
<h3 id="distributional-similarity">Distributional similarity</h3>
<p>The other different measure of similarity is using distributional similarity and collections. The intuition is that two wrods that frequently appears together are more likely to be semantically related. Like &quot;Play at zoo.&quot;, &quot;Play at amusement park&quot;. Both &quot;amusement park&quot; and &quot;zoo&quot; appears together with &quot;play&quot;. Therefore these two words could be closely related. We can define the range of context by words within a small window, or specific syntactic relation to the taget word, etc.</p>
<p>Once we have defined the context, we can find the strength of correlation. The criteria could be occurance frequency of a word. For example,&quot;is&quot; is a very frequent word, so it has high chance of co-occuring with other words. Therefore it should be paid less weight. One measure is <strong>Pointwise Mutual Information</strong></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>PMI(w,c) = log[P(w,c)/P(w)P(c)]</mtext></mrow><annotation encoding="application/x-tex">\text{PMI(w,c) = log[P(w,c)/P(w)P(c)]}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">PMI(w,c) = log[P(w,c)/P(w)P(c)]</span></span></span></span></span></span></p>
<p>where w is word of interes and c is the context word.</p>
<p>In Python, NLTK also provides such measure as follows.</p>
<pre><code>import nltk
from nltk.collocations import *

bigram_measures = nltk.collocations.BigramAssocMeasures()

# learn that based on corpus
finder = BigramCollocationFinder.From_words(text)
# get the top 10 pairs using the PMI measures from bigram measures
finder.nbest(bigram_measures.pmi, 10)

# finder also provides other functions
# such as frequency filter
finder.apply_freq_filter(10)
</code></pre>
<p>From the discussion above we find that finding similarity between words and text is important and WordNet is a useful tool to identify the semantic relationship. NLTK offers great assistance to such task.</p>
<h2 id="topic-modeling">Topic modeling</h2>
<p>Topic modeling is a coarse-level analysis of what is in a text collection. It is an exploratory tool used for text mining which can help us to have a general understanding of the text type. For example, is it about sports, or business, or pilitics.<br>
Topic modeling is a coarse-level analysis of what is in a text collection. It can help us to have a general understanding of the text type. For example, it can tell us if the text is about sports, or business, or pilitics.</p>
<p>What is known are the text corpus and number of topics. What is not known are the actual topics and the topic distribution for each document.</p>
<p>There are two common approaches:</p>
<ul>
<li>Probabilistic Latent Semantic Analysis (PLSA).</li>
<li>Latent Dirichlet Allocation (LDA). This is what we will further discuss about in this post.</li>
</ul>
<h2 id="lda">LDA</h2>
<p>LDA is a generative model used extensively for modeling large text corpora. We can use it as a first step to understand what the text is about.</p>
<p>The general stops of working with LDA in Python is as follows:</p>
<ol>
<li>Pre-processing the text</li>
</ol>
<ul>
<li>Tokenize and normalize (lowercase) the sentences.</li>
<li>Remove stop words</li>
<li>Stemming the words</li>
</ul>
<ol start="2">
<li>Convert tokenized documents to a document term matrix.</li>
<li>Build LDA models on the doc-term matrix.</li>
</ol>
<p>Suppose we have a set of pre-processed text documents <em>doc_set</em>. A general process is as follows:</p>
<pre><code>import gensim
from gensim import corpora, models

# create a dictionary, which is a mapping between IDs and words
dictionary = corpora.Dictionary(doc_set)

# Then create corpus which consists of all the words in doc_set
corpus = [dictionary.doc2bow(doc) for doc in doc_set]

# Create the document term matrix, then put in the LdaModel call
Idamodel = gensim.models.Idamodel.LdaModel(corpus, num_topics = 4, id2word = dictionary, passes = 50)

print(Idamodel.print_topics(num_topics = 4, num_words = 5))
</code></pre>
<h1 id="document-similarity-and-lda-modeling-in-python">Document similarity and LDA modeling in Python</h1>
<h2 id="build-similarity-score-function-from-scratch">Build similarity score function from scratch</h2>
<p>For the first part, we will make functions <code>doc_to_synsets</code> and <code>similarity_score</code> which will be used by <code>document_path_similarity</code> to find the path similarity between two documents..</p>
<ul>
<li><code>document_path_similarity:</code> computes the symmetrical path similarity between two documents by finding the synsets in each document using <code>doc_to_synsets</code>, then computing similarities using <code>similarity_score</code>.</li>
</ul>
<pre><code class="language-python">import nltk
import numpy as np
from nltk.corpus import wordnet as wn
import pandas as pd

# nltk.download('wordnet')
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
</code></pre>
<pre><code class="language-python">import numpy as np
import nltk
from nltk.corpus import wordnet as wn
import pandas as pd


def convert_tag(tag):
    &quot;&quot;&quot;Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets&quot;&quot;&quot;
    
    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}
    try:
        return tag_dict[tag[0]]
    except KeyError:
        return None


</code></pre>
<p><code>convert_tag:</code> converts the tag given by <code>nltk.pos_tag</code> to a tag used by <code>wordnet.synsets</code>. We will need to use this function in <code>doc_to_synsets</code>.</p>
<p>We want to first formulate following functions:</p>
<ul>
<li><code>doc_to_synsets:</code> returns a list of synsets in document. This function first tokenizes and part of speech tags the document using <code>nltk.word_tokenize</code> and <code>nltk.pos_tag</code>. Then it should find each tokens corresponding synset using <code>wn.synsets(token, wordnet_tag)</code>. The first synset match should be used. If there is no match, that token is skipped.</li>
<li><code>similarity_score:</code> returns the normalized similarity score of a list of synsets (s1) onto a second list of synsets (s2). For each synset in s1, find the synset in s2 with the largest similarity value. Sum all of the largest similarity values together and normalize this value by dividing it by the number of largest similarity values found. Missing values are ignored.</li>
<li><code>similarity_score:</code> returns a normalized similarity score between two documents.</li>
</ul>
<pre><code class="language-python">def doc_to_synsets(doc):
    token_text = nltk.word_tokenize(doc)
    # tokenize by nltk, which is covered in the previous NLTK post
    ps_tag = nltk.pos_tag(token_text)
    # store the converted tags
    con_tg = []
    # iterate over tuples in a list
    for index, tp in enumerate(ps_tag):
      word = tp[0]
      tg = tp[1]
      con_tg.insert(index, (word, convert_tag(tg)))
    # find all the synsets, given word w[0] and POS tag w[1]
    synset_list = [wn.synsets(w[0],w[1]) for w in con_tg]
    # get rid of None or empty list
    synset_list = [x for x in synset_list if x]

    # store the first synset match
    valid_synset_01 = []
    for i in range(len(synset_list)):
      valid_synset_01.append(synset_list[i][0])
    
    return valid_synset_01 
</code></pre>
<pre><code class="language-python">def similarity_score(s1, s2):

    &quot;&quot;&quot;
    Calculate the normalized similarity score of s1 onto s2, where s1 and s2 are two lists of synsets

    For each synset in s1, finds the synset in s2 with the largest similarity value.
    Sum of all of the largest similarity values and normalize this value by dividing it by the
    number of largest similarity values found.

    Args:
        s1, s2: list of synsets from doc_to_synsets

    Returns:
        normalized similarity score of s1 onto s2
    &quot;&quot;&quot;

    # store the largest similarity score
    score_largest = []
    for i in range(len(s2)):
      score_within =[]
      for k in range(len(s1)):
        score_within.append(s2[i].path_similarity(s1[k]))
      # if a list consists of 'None' only , the max() function does not work
      if all(x is None for x in score_within):
        score_largest.insert(i, 0)
      else:
        score_largest.insert(i, max(list(filter(None, score_within))))
    
    return sum(score_largest) / len(score_largest)
</code></pre>
<pre><code class="language-python">
def document_path_similarity(doc1, doc2):
    &quot;&quot;&quot;
    Finds the symmetrical similarity between doc1 and doc2
    
    Arg:
      doc1, doc2: sentences, text, or document

    Returns:
      a similarity measure, data type = float

    &quot;&quot;&quot;

    synsets1 = doc_to_synsets(doc1)
    synsets2 = doc_to_synsets(doc2)

    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2
</code></pre>
<p>Let us test if the fuctions work</p>
<pre><code class="language-python"># test
d1 = 'I like cats'
d2 = 'I like dogs and dolphines'
document_path_similarity(d1, d2)
</code></pre>
<pre><code>0.7333333333333334
</code></pre>
<p>It seems to be working well.</p>
<h2 id="apply-pre-defined-functions-to-text-data">Apply pre-defined functions to text data</h2>
<p>The data can be found <a href="https://drive.google.com/file/d/1O_RjmjjcOXD9Sv_9tw1ZDSHSPpnVYeaJ/view?usp=sharing">here</a>.</p>
<p>The file consists of three columns: <code>Quality</code>, <code>D1</code>, and <code>D2</code>. <code>Quality</code> is an indictor if two documents <code>D1</code> and <code>D2</code> are paraphrases of each other.</p>
<pre><code class="language-python"># import data from google drive
# use the following code if want to connect colab to google drive
from google.colab import drive

drive.mount('/content/drive')

</code></pre>
<pre><code>Mounted at /content/drive
</code></pre>
<pre><code class="language-python"># Use this dataframe for questions most_similar_docs and label_accuracy
paraphrases = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Applied_Text_Mining_in_Python/TopicModeling/paraphrases.csv')
paraphrases.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Quality</th>
      <th>D1</th>
      <th>D2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Ms Stewart, the chief executive, was not expec...</td>
      <td>Ms Stewart, 61, its chief executive officer an...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>After more than two years' detention under the...</td>
      <td>After more than two years in detention by the ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>"It still remains to be seen whether the reven...</td>
      <td>"It remains to be seen whether the revenue rec...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>And it's going to be a wild ride," said Allan ...</td>
      <td>Now the rest is just mechanical," said Allan H...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>The cards are issued by Mexico's consulates to...</td>
      <td>The card is issued by Mexico's consulates to i...</td>
    </tr>
  </tbody>
</table>
</div>
<p>Next, we are interested in getting two pieces of information:</p>
<ul>
<li><code>most_similar_docs:</code> the pair of documents in paraphrases which has the maximum similarity score.</li>
<li><code>label_accuracy:</code> find labels for the twenty pairs of documents by computing the similarity for each pair using <code>document_path_similarity</code>. Let the classifier rule be that if the score is greater than 0.75, label is paraphrase (1), else label is not paraphrase (0).</li>
</ul>
<pre><code class="language-python">import operator
def most_similar_docs():
  similarity_score_i = []
  for i in range(len(paraphrases)):
    d1 = paraphrases.iloc[i]['D1']
    d2 = paraphrases.iloc[i]['D2']
    # similarity_score_i.insert(i,document_path_similarity(d1,d2))
    similarity_score_i.insert(i,(d1,d2,document_path_similarity(d1,d2)))
    # sort by the 3rd column, which is the similarity score
  similarity_score_i = sorted(similarity_score_i, key=operator.itemgetter(2), reverse = True)
    
  return similarity_score_i[0]
</code></pre>
<pre><code class="language-python">print('The most similar paraphrases and corresponding similarity score are: \n {}'.format(most_similar_docs()))
</code></pre>
<pre><code>The most similar paraphrases and corresponding similarity score are: 
 ('&quot;Indeed, Iran should be put on notice that efforts to try to remake Iraq in their image will be aggressively put down,&quot; he said.', '&quot;Iran should be on notice that attempts to remake Iraq in Iran\'s image will be aggressively put down,&quot; he said.\n', 0.9502923976608186)
</code></pre>
<pre><code class="language-python">def label_accuracy():
    from sklearn.metrics import accuracy_score
    paraphrases_score = paraphrases.copy()
    for i in range(len(paraphrases)):
      d1 = paraphrases.iloc[i]['D1']
      d2 = paraphrases.iloc[i]['D2']
      paraphrases_score.loc[i,[&quot;similarity_score&quot;]] = document_path_similarity(d1,d2)
    # Your Code Here
    paraphrases_score['LabelByScore'] = np.where(paraphrases_score['similarity_score'] &gt; 0.75,1,0)
    return accuracy_score(paraphrases_score['Quality'], paraphrases_score['LabelByScore'])
</code></pre>
<pre><code class="language-python">print('The accuracy is: {}'.format(label_accuracy()))
</code></pre>
<pre><code>The accuracy is: 0.7
</code></pre>
<h2 id="topic-modeling-2">Topic Modeling</h2>
<p>Here we will use Gensim's LDA (Latent Dirichlet Allocation) model to model topics in <a href="https://drive.google.com/file/d/1U6LI2szI5-cuuNrMgQfpuOc29lGeORc5/view?usp=sharing">newsgroup_data</a>.</p>
<pre><code class="language-python">import pickle
import gensim
from sklearn.feature_extraction.text import CountVectorizer

# Load the list of documents
with open('/content/drive/MyDrive/Colab Notebooks/Applied_Text_Mining_in_Python/TopicModeling/newsgroups', 'rb') as f:
    newsgroup_data = pickle.load(f)

# Use CountVectorizor to find three or more letter tokens, remove stop_words, 
# remove tokens that don't appear in at least 20 documents,
# remove tokens that appear in more than 20% of the documents
vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', 
                       token_pattern=(r'\b\w\w\w+\b'))
# Fit and transform the input data
X = vect.fit_transform(newsgroup_data)

# Convert sparse matrix to gensim corpus.
corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)

# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)
id_map = dict((v, k) for k, v in vect.vocabulary_.items())

</code></pre>
<pre><code class="language-python">len(vect.vocabulary_.items())
</code></pre>
<pre><code>901
</code></pre>
<pre><code class="language-python"># Use the gensim.models.LdaModel constructor to estimate 
# LDA model parameters on the corpus, and save to the variable `ldamodel`

from gensim import corpora, models
# from gensim.models import Idamodel
ldamodel = gensim.models.LdaModel(corpus, num_topics = 10, id2word = id_map, passes = 25, random_state = 34)
</code></pre>
<pre><code class="language-python">print(ldamodel.print_topics(num_topics = 10, num_words = 10))
</code></pre>
<pre><code>[(0, '0.056*&quot;edu&quot; + 0.043*&quot;com&quot; + 0.033*&quot;thanks&quot; + 0.022*&quot;mail&quot; + 0.021*&quot;know&quot; + 0.020*&quot;does&quot; + 0.014*&quot;info&quot; + 0.012*&quot;monitor&quot; + 0.010*&quot;looking&quot; + 0.010*&quot;don&quot;'), (1, '0.024*&quot;ground&quot; + 0.018*&quot;current&quot; + 0.018*&quot;just&quot; + 0.013*&quot;want&quot; + 0.013*&quot;use&quot; + 0.011*&quot;using&quot; + 0.011*&quot;used&quot; + 0.010*&quot;power&quot; + 0.010*&quot;speed&quot; + 0.010*&quot;output&quot;'), (2, '0.061*&quot;drive&quot; + 0.042*&quot;disk&quot; + 0.033*&quot;scsi&quot; + 0.030*&quot;drives&quot; + 0.028*&quot;hard&quot; + 0.028*&quot;controller&quot; + 0.027*&quot;card&quot; + 0.020*&quot;rom&quot; + 0.018*&quot;floppy&quot; + 0.017*&quot;bus&quot;'), (3, '0.023*&quot;time&quot; + 0.015*&quot;atheism&quot; + 0.014*&quot;list&quot; + 0.013*&quot;left&quot; + 0.012*&quot;alt&quot; + 0.012*&quot;faq&quot; + 0.012*&quot;probably&quot; + 0.011*&quot;know&quot; + 0.011*&quot;send&quot; + 0.010*&quot;months&quot;'), (4, '0.025*&quot;car&quot; + 0.016*&quot;just&quot; + 0.014*&quot;don&quot; + 0.014*&quot;bike&quot; + 0.012*&quot;good&quot; + 0.011*&quot;new&quot; + 0.011*&quot;think&quot; + 0.010*&quot;year&quot; + 0.010*&quot;cars&quot; + 0.010*&quot;time&quot;'), (5, '0.030*&quot;game&quot; + 0.027*&quot;team&quot; + 0.023*&quot;year&quot; + 0.017*&quot;games&quot; + 0.016*&quot;play&quot; + 0.012*&quot;season&quot; + 0.012*&quot;players&quot; + 0.012*&quot;win&quot; + 0.011*&quot;hockey&quot; + 0.011*&quot;good&quot;'), (6, '0.017*&quot;information&quot; + 0.014*&quot;help&quot; + 0.014*&quot;medical&quot; + 0.012*&quot;new&quot; + 0.012*&quot;use&quot; + 0.012*&quot;000&quot; + 0.012*&quot;research&quot; + 0.011*&quot;university&quot; + 0.010*&quot;number&quot; + 0.010*&quot;program&quot;'), (7, '0.022*&quot;don&quot; + 0.021*&quot;people&quot; + 0.018*&quot;think&quot; + 0.017*&quot;just&quot; + 0.012*&quot;say&quot; + 0.011*&quot;know&quot; + 0.011*&quot;does&quot; + 0.011*&quot;good&quot; + 0.010*&quot;god&quot; + 0.009*&quot;way&quot;'), (8, '0.034*&quot;use&quot; + 0.023*&quot;apple&quot; + 0.020*&quot;power&quot; + 0.016*&quot;time&quot; + 0.015*&quot;data&quot; + 0.015*&quot;software&quot; + 0.012*&quot;pin&quot; + 0.012*&quot;memory&quot; + 0.012*&quot;simms&quot; + 0.012*&quot;port&quot;'), (9, '0.068*&quot;space&quot; + 0.036*&quot;nasa&quot; + 0.021*&quot;science&quot; + 0.020*&quot;edu&quot; + 0.019*&quot;data&quot; + 0.017*&quot;shuttle&quot; + 0.015*&quot;launch&quot; + 0.015*&quot;available&quot; + 0.014*&quot;center&quot; + 0.014*&quot;sci&quot;')]
</code></pre>
<p>From the ouput above we can see the words forming the ten topics.</p>
<p>Next question could be, what if we add in a new document?</p>
<pre><code class="language-python">new_doc = [&quot;\n\n Today is a long day. \
I have not eaten dinner since 8 pm. \
There are too many tasks to do. \n\n\
Bolin\n-- &quot;]
</code></pre>
<pre><code class="language-python">X_new = vect.transform(new_doc)
corpus_new = gensim.matutils.Sparse2Corpus(X, documents_columns=False)
</code></pre>
<pre><code class="language-python"># Take a look at how a part of distribution looks like
list(ldamodel.get_document_topics(corpus_new))[:5]
</code></pre>
<pre><code>[[(6, 0.9470456)],
 [(3, 0.48242074), (5, 0.40150425), (6, 0.096070684)],
 [(4, 0.9139199), (8, 0.057501744)],
 [(0, 0.030418986), (5, 0.10291002), (6, 0.8162316), (8, 0.04535334)],
 [(0, 0.020000072),
  (1, 0.8199883),
  (2, 0.020000583),
  (3, 0.020001208),
  (4, 0.02000103),
  (5, 0.020001303),
  (6, 0.020001443),
  (7, 0.020002373),
  (8, 0.02000331),
  (9, 0.020000374)]]
</code></pre>
<h1 id="ending">Ending</h1>
<p>So far I have shared some basic concepts of similarity measures and application of Topic Modeling.</p>
<p>Several points notice while learning are that</p>
<ul>
<li>When the number of topics are large, it takes a long time to train the LDA model.</li>
<li>The LDA seems to regard every token induvidually, not taking the factor of context into account. Therefore there is still space for developping this model's explanatory power.</li>
<li>RegEx is foundamental in NLP tasts. When passing parameters to CountVectorizer function, we can specify the token_pattern through RegEx. Though it is trivial, it can facilitate the further analysis greatly.</li>
</ul>
<p>I hope this post could be helpful for you. If there is any question please let me know.</p>
<p>Cheers!</p>

        </div>
        <!-- Share to Twitter, Weibo, Telegram -->
        <div class="flex items-center">
          <div class="mr-4 flex items-center">
            <i class="ri-share-forward-line text-gray-500"></i>
          </div>
          <div class="px-4 cursor-pointer text-blue-500 hover:bg-blue-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTwitter">
            <i class="ri-twitter-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-red-500 hover:bg-red-100 dark:hover:bg-gray-600 inline-flex" @click="shareToWeibo">
            <i class="ri-weibo-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-indigo-500 hover:bg-indigo-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTelegram">
            <i class="ri-telegram-line"></i>
          </div>
        </div>
      </div>

      

      

      <footer class="py-12 text-center px-4 md:px-0" v-pre>
  Prudence is a fountain of life to the prudent.
</footer>
    </div>

    <!-- TOC Container -->
    <div class="fixed right-0 bottom-0 mb-16 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white dark:bg-gray-500 dark:text-gray-200 hover:shadow-lg transition-all animated fadeInRight" @click="showToc = true">
      <i class="ri-file-list-line"></i>
    </div>

    <div class="fixed right-0 top-0 bottom-0 overflow-y-auto w-64 bg-white dark:bg-gray-800 p-4 border-l border-gray-100 dark:border-gray-600 z-10 transition-fast" :class="{ '-mr-64': !showToc }">
      <div class="flex mb-4 justify-end">
        <div class="w-8 h-8 inline-flex justify-center items-center rounded-full cursor-pointer hover:bg-gray-200 dark:hover:bg-gray-600 transition-fast" @click="showToc = false">
          <i class="ri-close-line text-lg"></i>
        </div>
      </div>
      <div class="post-toc-container">
        <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#semantic-text-similarity">Semantic text similarity</a>
<ul>
<li><a href="#wordnet">WordNet</a></li>
<li><a href="#distributional-similarity">Distributional similarity</a></li>
</ul>
</li>
<li><a href="#topic-modeling">Topic modeling</a></li>
<li><a href="#lda">LDA</a></li>
</ul>
</li>
<li><a href="#document-similarity-and-lda-modeling-in-python">Document similarity and LDA modeling in Python</a>
<ul>
<li><a href="#build-similarity-score-function-from-scratch">Build similarity score function from scratch</a></li>
<li><a href="#apply-pre-defined-functions-to-text-data">Apply pre-defined functions to text data</a></li>
<li><a href="#topic-modeling-2">Topic Modeling</a></li>
</ul>
</li>
<li><a href="#ending">Ending</a></li>
</ul>

      </div>
    </div>

    <!-- Back to top -->
    <div class="fixed right-0 bottom-0 mb-4 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white hover:shadow-lg transition-all dark:bg-gray-500 dark:text-gray-200" @click="backToUp" v-show="scrolled">
      <i class="ri-arrow-up-line"></i>
    </div>
  </div>

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
  <!-- Background of PhotoSwipe. 
        It's a separate element as animating opacity is faster than rgba(). -->
  <div class="pswp__bg">
  </div>
  <!-- Slides wrapper with overflow:hidden. -->
  <div class="pswp__scroll-wrap">
    <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
    <div class="pswp__container">
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
    </div>
    <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
    <div class="pswp__ui pswp__ui--hidden">
      <div class="pswp__top-bar">
        <!--  Controls are self-explanatory. Order can be changed. -->
        <div class="pswp__counter">
        </div>
        <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
        <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
        <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
        <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
        <!-- element will get class pswp__preloader--active when preloader is running -->
        <div class="pswp__preloader">
          <div class="pswp__preloader__icn">
            <div class="pswp__preloader__cut">
              <div class="pswp__preloader__donut">
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
        <div class="pswp__share-tooltip">
        </div>
      </div>
      <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
      </button>
      <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
      </button>
      <div class="pswp__caption">
        <div class="pswp__caption__center">
        </div>
      </div>
    </div>
  </div>
</div>

  <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
  <script src="https://BolinWu-Gridea.github.io/media/scripts/main.js"></script>
  
  <!-- Code Highlight -->
  
    <script src="https://BolinWu-Gridea.github.io/media/prism.js"></script>
    <script>
      Prism.highlightAll()
    </script>
  

  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>
  <script>
    //拿到预览框架，也就是上面的html代码
    var pswpElement = document.querySelectorAll('.pswp')[0];
    //定义图片数组变量
    var imgitems;
    /**
    * 用于显示预览界面
    * @param index 图片数组下标
    */
    function viewImg(index) {
      //其它选项这里不做过多阐述，详情见官网
      var pswpoptions = {
        index: parseInt(index, 10), // 开始幻灯片索引。0是第一张幻灯片。必须是整数，而不是字符串。
        bgOpacity: 0.7, // 背景透明度，0-1
        maxSpreadZoom: 3, // 缩放级别，不要太大
      };
      //初始化并打开PhotoSwipe，pswpElement对应上面预览框架，PhotoSwipeUI_Default为皮肤，imgitems为图片数组，pswpoptions为选项
      var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, imgitems, pswpoptions);
      gallery.init()
    }
    /**
    * 用于添加图片点击事件
    * @param img 图片元素
    * @param index 所属下标（在imgitems中的位置）
    */
    function addImgClick(img, index) {
      img.onclick = function() {
        viewImg(index)
      }
    }
    /**
    * 轮询所有图片，获取src、width、height等数据，加入imgitems，并给图片元素添加事件
    * 最好在onload中执行该方法，本站因放在最底部，所以直接初始化
    * 异步加载图片可在图片元素创建完成后调用此方法
    */
    function initImg() {
      //重置图片数组
      imgitems = [];
      //查找class:markdown 下的所有img元素并遍历
      var imgs = document.querySelectorAll('.markdown img');
      for (var i = 0; i < imgs.length; i++) {
        var img = imgs[i];
        //本站相册初始为loading图片，真实图片放在data-src
        var ds = img.getAttribute("data-src");
        //创建image对象，用于获取图片宽高
        var imgtemp = new Image();
        //判断是否存在data-src
        if (ds != null && ds.length > 0) {
          imgtemp.src = ds
        } else {
          imgtemp.src = img.src
        }
        //判断是否存在缓存
        if (imgtemp.complete) {
          var imgobj = {
            "src": imgtemp.src,
            "w": imgtemp.width,
            "h": imgtemp.height,
          };
          imgitems[i] = imgobj;
          addImgClick(img, i);
        } else {
          console.log('进来了2')
          imgtemp.index = i;
          imgtemp.img = img;
          imgtemp.onload = function() {
            var imgobj = {
              "src": this.src,
              "w": this.width,
              "h": this.height,
            };
            //不要使用push，因为onload前后顺序会不同
            imgitems[this.index] = imgobj
            //添加点击事件
            addImgClick(this.img, this.index);
          }
        }
      }
    }
    //初始化
    initImg();
  </script>
  
  
</body>

</html>