<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Classification of Image Data by Simple NN  - Bolin Wu</title>
<link rel="shortcut icon" href="https://BolinWu-Gridea.github.io/favicon.ico">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css">
<link rel="stylesheet" href="https://BolinWu-Gridea.github.io/media/css/tailwind.css">
<link rel="stylesheet" href="https://BolinWu-Gridea.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="Classification of Image Data by Simple NN  - Bolin Wu - Atom Feed" href="https://BolinWu-Gridea.github.io/atom.xml">

    

  <meta name="description" content="
Intuitively, simple neural network is a combination of many (linear) transformations, which is similar to mixture model..." />
  <meta property="og:title" content="Classification of Image Data by Simple NN  - Bolin Wu">
  <meta property="og:description" content="
Intuitively, simple neural network is a combination of many (linear) transformations, which is similar to mixture model..." />
  <meta property="og:type" content="articles">
  <meta property="og:url" content="https://BolinWu-Gridea.github.io/post/2021-02-09-SimpleNN/" />
  <meta property="og:image" content="https://BolinWu-Gridea.github.io/post-images/2021-02-09-SimpleNN.jpg">
  <meta property="og:image:height" content="630">
  <meta property="og:image:width" content="1200">
  <meta name="twitter:title" content="Classification of Image Data by Simple NN  - Bolin Wu">
  <meta name="twitter:description" content="
Intuitively, simple neural network is a combination of many (linear) transformations, which is similar to mixture model...">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="canonical" href="https://BolinWu-Gridea.github.io/post/2021-02-09-SimpleNN/">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
 
  
    <link rel="stylesheet" href="https://BolinWu-Gridea.github.io/media/css/prism-atom-dark.css">
  

  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
  
</head>

<body>
  <div class="antialiased flex flex-col min-h-screen" id="app">
    <a href="https://BolinWu-Gridea.github.io" class="fixed top-0 left-0 mt-4 bg-black text-white dark:text-gray-700 dark:bg-yellow-50 dark:hover:bg-black dark:hover:text-white inline-flex p-2 pl-8 hover:text-gray-700 hover:bg-yellow-50 font-bold z-10 transition-fast animated fadeInLeft">
      Bolin Wu
    </a>
    <div class="max-w-4xl w-full mx-auto">
      <div class="shadow-box bg-white dark:bg-gray-600 rounded-lg pt-32 md:pt-64 px-4 md:px-8 pb-8 animated fadeIn mb-8">
        <h1 class="text-5xl font-semibold leading-normal pb-8 mb-8 border-b-8 border-gray-700">
          Classification of Image Data by Simple NN 
        </h1>
        
          <img src="https://BolinWu-Gridea.github.io/post-images/2021-02-09-SimpleNN.jpg" alt="Classification of Image Data by Simple NN " class="block w-full mb-8">
        
        <div class="mb-8 flex flex-wrap">
          <div class="text-gray-400 text-sm mr-4">2021-02-07 · 16 min read</div>
          
            <a href="https://BolinWu-Gridea.github.io/tag/g7o3VtjDf5/" class="text-gray-700 text-sm border-b-2 border-dotted border-gray-200 hover:border-gray-600 transition-all duration-100 inline-flex mr-2">
              <i class="ri-hashtag"></i>
              supervised learning
            </a>
          
        </div>
        <div class="markdown mb-8" v-pre>
          <!-- more -->
<p>Intuitively, simple neural network is a combination of many (linear) transformations, which is similar to mixture model in some way. It allows to transform the input data in a more sophisticated way that a single linear model could not achieve. Simple neural network is the foundation for many other more advanced neural network models e.g., Recurrent Neural Network and Long Short Term Memory (LSTM). By the way, I posted a project of LSTM <a href="https://bolinwu-gridea.github.io/post/2020-01-14-ML-Wine/">here</a> please feel free to check it out if you are interested.</p>
<p>If you have not heard of neural network before, this video may help you to easily grasp the idea:<br>
<a href="http://www.youtube.com/watch?v=CqOfi41LfDw" title="StatQuest"><img src="http://img.youtube.com/vi/CqOfi41LfDw/0.jpg" alt="" loading="lazy"></a></p>
<p>The content of this post includes:</p>
<ol>
<li>The basics of <strong>feedforward neural network</strong>.</li>
<li>The application of it with the help of TensorFlow and Keras.</li>
<li>Several useful parameter tunings.</li>
</ol>
<p>The main reference is <a href="https://www.deeplearningbook.org">Deep Learning, Goodfellow et al</a>, Chapter 6.</p>
<h1 id="neural-network-basics">Neural Network Basics</h1>
<p>At first, let us implement a straight forward neural network mathematically. We assume our single hidden layer network to be:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>W</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo>)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msup><mi>W</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>c</mi><mo>)</mo><mo>+</mo><mi>b</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
f(x,W,c,w,b) = w^{T} max(0,W^{T}x+c)+b
\end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.551331em;vertical-align:-0.5256654999999999em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0256655000000001em;"><span style="top:-3.1343345000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5256654999999999em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>Where X is the input data, W,w,are the weight matrix, c,b are the interceps, for hidden layer and output layer respectively. These parameters are usually estimated by the backpropagation algorithm. Here for illustration we just assume:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">W = \begin{bmatrix}1 &amp; 1\\
1 &amp; 1
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">c = \begin{bmatrix}0 \\
-1
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mn>2</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">w = \begin{bmatrix}1 \\
-2
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span></span></p>
<p>and b =0.</p>
<p>The input data:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">X = \begin{bmatrix}0 &amp; 0\\
0 &amp; 1\\
1 &amp; 0\\
1 &amp; 1
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.80204em;vertical-align:-2.15003em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6520099999999998em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-2.80499em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-3.40599em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎢</span></span></span><span style="top:-4.65201em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6500000000000004em;"><span style="top:-4.8100000000000005em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-1.2099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1500000000000004em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6520099999999998em;"><span style="top:-1.6499900000000003em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-2.80499em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-3.40599em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎥</span></span></span><span style="top:-4.65201em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.15003em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>The basic steps of implementing the network above:</p>
<ol>
<li>Transform the original data X by using weight W and intercept c.</li>
<li>Send the transformed data into activation function, get output X'.</li>
<li>Transform the X' by using the weight w and intercept b. Then get the final output.</li>
</ol>
<p>The max() function part is actually a ReLU activation function and this example is a realization of XOR logic gate.</p>
<p>R Code:</p>
<pre><code class="language-r"># load the package
library(sigmoid)
library(keras)
library(kerasR)
library(tensorflow)

W = matrix(rep(1,4),nrow = 2); c = matrix(c(0,-1), nrow = 2);
w = matrix(c(1,-2), nrow = 2)
X = matrix(c(0,0,0,1,1,0,1,1), byrow = T, nrow = 4, ncol = 2)

network = function(x_input,W_input, c_input, w_input, b_input){
  # transfor c_input so that it can be used for addition in the next step
  c_trans = matrix(NA, nrow =  nrow(x_input %*% W_input), ncol = nrow(c_input))
  for (i in 1: nrow(c_input) ) {
    c_trans[,i] = rep(c_input[i],nrow(x_input %*% W_input) )

  }

  # (6.8) and (6.9)
  layer_trans1  = x_input %*% W_input + c_trans
  # put into activation function, (6.10)
  activation_1 = relu(layer_trans1)
  # output, (6.11)
  out_1 = activation_1 %*% w + b_input
  return(out_1)

}


network(X,W_input = W,c_input = c,w_input = w,b_input =0)

     [,1]
[1,]    0
[2,]    1
[3,]    1
[4,]    0
</code></pre>
<h1 id="feed-forward-neural-network-using-keras-and-tensorflow">Feed-Forward Neural Network using Keras and TensorFlow</h1>
<p>Now let us step into the application part. You can find find how to install keras and tensorflow in R <a href="https://tensorflow.rstudio.com/installation/">here</a>.<br>
The data that we will use is the classic MNIST dataset from keras which contains a huge amount of hand-written digits. The data could be easiliy loaded as follows:</p>
<pre><code class="language-r">library(keras)
mnist &lt;- dataset_mnist()
# scale the dataset
mnist$train$x &lt;- mnist$train$x/255
mnist$test$x &lt;- mnist$test$x/255
</code></pre>
<h2 id="1-have-a-grasp-of-mnist">1. Have a grasp of MNIST</h2>
<p>Let us first have a grasp about the dataset by visualizing a digit. You could play around the code and explore the dataset in your own way.</p>
<pre><code class="language-r">idx &lt;- 3
im &lt;- mnist$train$x[idx,,]
# Transpose the image
im &lt;- t(apply(im, 2, rev))
image(1:28, 1:28, im, col=gray((0:255)/255), xlab = &quot;&quot;, ylab = &quot;&quot;,
        xaxt='n', yaxt='n', main=paste(mnist$train$y[idx]))
</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680342589.png" alt="" loading="lazy"><br>
<em>Fig.1 Digit Visualization</em></p>
<p>And we can also use object.size()function to see the size of MNIST dataset:</p>
<pre><code class="language-r">cat(&quot;The training data set is&quot;,object.size(mnist$train),&quot;bytes;&quot;,&quot;\n&quot;,
    &quot;The test data set is&quot;,object.size(mnist$test),&quot;bytes.&quot;)

The training data set is 376560792 bytes;
 The test data set is 62760792 bytes.
</code></pre>
<h2 id="2-implementation">2. Implementation</h2>
<p>Next, we will start with on hidden layer with 16 units and the sigmoid as the avtivation function, without any regularization.</p>
<pre><code class="language-r"># set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%
  layer_dense(units = 16, activation = &quot;sigmoid&quot;)
  %&gt;%
# ouput layer
  layer_dense(10, activation = &quot;softmax&quot;)

# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 2
  )

Epoch 1/5
1313/1313 - 1s - loss: 0.5673 - accuracy: 0.8667
1313/1313 - 2s - loss: 0.5673 - accuracy: 0.8667 - val_loss: 0.3418 - val_accuracy: 0.9087
Epoch 2/5
1313/1313 - 1s - loss: 0.3237 - accuracy: 0.9125
1313/1313 - 2s - loss: 0.3237 - accuracy: 0.9125 - val_loss: 0.3124 - val_accuracy: 0.9139
Epoch 3/5
1313/1313 - 1s - loss: 0.2981 - accuracy: 0.9174
1313/1313 - 2s - loss: 0.2981 - accuracy: 0.9174 - val_loss: 0.2998 - val_accuracy: 0.9169
Epoch 4/5
1313/1313 - 1s - loss: 0.2841 - accuracy: 0.9212
1313/1313 - 2s - loss: 0.2841 - accuracy: 0.9212 - val_loss: 0.2951 - val_accuracy: 0.9199
Epoch 5/5
1313/1313 - 1s - loss: 0.2753 - accuracy: 0.9235
1313/1313 - 2s - loss: 0.2753 - accuracy: 0.9235 - val_loss: 0.2874 - val_accuracy: 0.9236

</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680418771.jpeg" alt="" loading="lazy"><br>
<em>Training Process</em></p>
<p>The accuracy after 5 epochs is 0.9235.</p>
<p>Please note that in the setup part, the last dense layer is an ouput layer. Its units number has to be 10 because there are ten digits and we are interested to classyfy these 10 circumstances. If in the future we would like to do yes/no classification, then the unit should be 1.</p>
<h2 id="3-parameter-tuning">3. Parameter Tuning</h2>
<p>The default epoch number is 5, hidden layer is 16 units and the activation function is sigmoid.</p>
<h3 id="a-increase-the-number-of-hidden-units-to-128">(a) Increase the number of hidden units to 128</h3>
<pre><code class="language-r"># set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%
  layer_dense(units = 128, activation = &quot;sigmoid&quot;) %&gt;% # increase unit to 128
  layer_dense(10, activation = &quot;softmax&quot;)

# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 2
  )


</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680446248.jpeg" alt="" loading="lazy"><br>
<em>Training Process</em></p>
<p>The validation accuracy after 5 epochs is around 0.9607.</p>
<h3 id="b-change-the-activation-function-to-relu">(b) Change the activation function to reLU</h3>
<pre><code class="language-r"># set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% # change to ReLU
  layer_dense(10, activation = &quot;softmax&quot;)

# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 2
  )


</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680485454.jpeg" alt="" loading="lazy"><br>
<em>Training Process</em></p>
<p>The validation accuracy after 5 epochs is around 0.9708.</p>
<h3 id="c-change-the-optimizer-to-rmsprop">(c) Change the optimizer to RMSprop</h3>
<p>Sebastian Ruder has an excellent papper <a href="https://arxiv.org/abs/1609.04747">An overview of gradient descent optimization algorithms</a> which illustrates the different optimizers. If you would like to know what are optimizers and the differences between them, please take a look.</p>
<pre><code class="language-r"># set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)


# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;RMSprop&quot;, # change to RMSprop
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 2
  )
</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680508104.jpeg" alt="" loading="lazy"><br>
<em>Training Process</em></p>
<p>The validation accuracy after 5 epochs is around 0.9678, not changed much.</p>
<h3 id="d-try-to-run-the-net-for-ten-epochs-and-use-early-stopping-for-regularization">(d) Try to run the net for ten epochs and use early stopping for regularization</h3>
<p>The early stopping means that the process stops training when a monitored metric has stopped improving. We can do so by adding <em>&quot;callbacks = callback_early_stopping(monitor = &quot;val_loss&quot;,patience = 3)&quot;</em> in the model fitting sequence as is shown above. The <em>&quot;patience&quot;</em> parameter is the number of epochs with no improvement after which training will be stopped.</p>
<p>The early stopping is one of the remedies for overfitting.</p>
<pre><code class="language-r"># set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)


# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;RMSprop&quot;, # change to RMSprop
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 10,
    validation_split = 0.3,
    verbose = 2,
      # early stopping
    callbacks = callback_early_stopping(monitor = &quot;val_loss&quot;,patience = 3)
  )

</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680558886.jpeg" alt="" loading="lazy"><br>
<em>Training Process</em></p>
<p>The validation accuracy after 5 epochs is around 0.9716 which is slightly improved.</p>
<h3 id="e-add-a-second-layer-with-128-hidden-units">(e) Add a second layer with 128 hidden units.</h3>
<pre><code class="language-r"># set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% # add a second layer
  layer_dense(10, activation = &quot;softmax&quot;)
 # add a second layer

# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;RMSprop&quot;,
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 10,
    validation_split = 0.3,
    verbose = 2
  )

summary(model)

Model: &quot;sequential_15&quot;
_________________________________________________________________________________
Layer (type)                        Output Shape                    Param #      
=================================================================================
flatten_15 (Flatten)                (None, 784)                     0            
_________________________________________________________________________________
dense_37 (Dense)                    (None, 128)                     100480       
_________________________________________________________________________________
dense_38 (Dense)                    (None, 128)                     16512        
_________________________________________________________________________________
dense_39 (Dense)                    (None, 10)                      1290         
=================================================================================
Total params: 118,282
Trainable params: 118,282
Non-trainable params: 0
_________________________________________________________________________________

</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680577971.jpeg" alt="" loading="lazy"><br>
<em>Training Process</em></p>
<p>The validation accuracy after 10 epochs is around 0.9750. If we use the summary function we can see that the number of total parameters with two hidden layers is 118,282, which is pretty huge compared with normal statistical models. One benefit is that it gives a pretty high classification accuracy, but the disadvantages could be the long time consumption of model training as well as potential overfitting problem. Next I will introduce <strong>dropout</strong> which could be a remedy for the mentioned disadvantages.</p>
<h3 id="f-add-dropout">(f) Add dropout.</h3>
<p>In practice you could choose what layers you want to implement dropout. Here I introduce dropout (p=0.2) to the first layer and dropout (p=0.5) to the second layer.</p>
<p>Without dropout, <strong>every</strong> node in a hidden layer is connected with <strong>every</strong> node in the next hidden layer. With dropout,the nodes in a hidden layer will be <strong>excluded</strong> with a given probability therefore it will fasten the training process as well as preventing overfitting to some extent.</p>
<pre><code class="language-r"># set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.2) %&gt;%  # introduce dropout
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.5) %&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)


# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;RMSprop&quot;,
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 10,
    validation_split = 0.3,
    verbose = 2
  )

</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621680598685.jpeg" alt="" loading="lazy"><br>
<em>Training Process</em></p>
<p>The validation accuracy after 10 epochs is around 0.9701.</p>
<h2 id="3-try-to-improve-the-network-architecture">3. Try to improve the network architecture</h2>
<p>Now, with all the tuning methods given above, you could play round and build a network that gives best accuracy.</p>
<p>After many trials of different parameters, I found the network with the best validation accuracy = 0.9725.<br>
The architecture is:</p>
<ol>
<li>Two hidden layers with the first activation function to be sigmoid and the second to be softmax. Both units are 128. The output layer has softmax activation function with 10 units.</li>
<li>Add drop out with 0.3 probability for each hidden layer. This can help accelerate the training process and prevent overfitting to some extent.</li>
<li>The optimizer is Adam method.</li>
<li>Set epochs = 50 and use early stopping (patiece = 3). I increase the number of epoch in case that the net work fails to get to its optimal model for lack of iteration. And the early stopping is used so that it can stop when its accuracy stop growing for three consecutive epochs.</li>
</ol>
<p>The code is listed below:</p>
<pre><code class="language-r"># change apoch, drop layout, add early stopping
# add another layer
# library(kerasR)
# set up the model
model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28, 28)) %&gt;%

  layer_dense(units = 128, activation = &quot;sigmoid&quot;) %&gt;%
  layer_dropout(0.3)%&gt;%
  layer_dense(units = 128, activation = &quot;softmax&quot;) %&gt;%
  layer_dropout(0.3)%&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)


# compile the model
model %&gt;%
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )

# fit the model
model %&gt;%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 50,
    validation_split = 0.3,
    verbose = 2,
    callbacks = callback_early_stopping(monitor = &quot;val_loss&quot;,patience = 3)
  )

</code></pre>
<h2 id="compute-the-accuracy-precision-and-recall-of-the-designed-model">Compute the accuracy, precision, and recall of the designed model</h2>
<p>Here we can use evaluate() funciton to get the accuracy directly. <br>
And to get precision and recall, I need to first use predict_classes() funciton to get the prediction in integer, and then use confutionMatrix in caret package.</p>
<p>The accuracy = 0.9741;</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:right">Precision</th>
<th style="text-align:right">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Class:0</td>
<td style="text-align:right">0.9807</td>
<td style="text-align:right">0.9867</td>
</tr>
<tr>
<td style="text-align:left">Class: 1</td>
<td style="text-align:right">0.9852</td>
<td style="text-align:right">0.9938</td>
</tr>
<tr>
<td style="text-align:left">Class: 2</td>
<td style="text-align:right">0.9748</td>
<td style="text-align:right">0.9748</td>
</tr>
<tr>
<td style="text-align:left">Class: 3</td>
<td style="text-align:right">0.9546</td>
<td style="text-align:right">0.9792</td>
</tr>
<tr>
<td style="text-align:left">Class: 4</td>
<td style="text-align:right">0.9845</td>
<td style="text-align:right">0.9715</td>
</tr>
<tr>
<td style="text-align:left">Class: 5</td>
<td style="text-align:right">0.9664</td>
<td style="text-align:right">0.9686</td>
</tr>
<tr>
<td style="text-align:left">Class: 6</td>
<td style="text-align:right">0.9792</td>
<td style="text-align:right">0.9812</td>
</tr>
<tr>
<td style="text-align:left">Class: 7</td>
<td style="text-align:right">0.9718</td>
<td style="text-align:right">0.9718</td>
</tr>
<tr>
<td style="text-align:left">Class: 8</td>
<td style="text-align:right">0.9733</td>
<td style="text-align:right">0.9713</td>
</tr>
<tr>
<td style="text-align:left">Class: 9</td>
<td style="text-align:right">0.9806</td>
<td style="text-align:right">0.9504</td>
</tr>
</tbody>
</table>
<h1 id="ending">Ending</h1>
<h2 id="challenges">Challenges</h2>
<p>In the end I would like to share some challenges that I met when I was implementing neural network (NN). <br>
First, understanding the feedforward NN structure in tensorflow. Before I ommited the fact that the last dense layer should be output layer and thought it was set up by the API by default. It is important to read documentation carefully. <br>
Second, we only need to use pipeline when compiling and fitting model. I did not understand why in the compliling part and fitting part, only pipeline is needed but we do not need to store it. I guess the reason could be that the tensorflow API is developped by Python or some other language as foundation therefore it does not follow our intuition of using R.</p>
<pre><code class="language-r">model  %&gt;% compile()
model %&gt;% fit()
# but we do not need to use
model = model  %&gt;% compile()
# or
model = model %&gt;% fit()
</code></pre>
<h2 id="tips">Tips</h2>
<ul>
<li>This post does not discuss some important concepts e.g., backpropagation and gradient descent, but they are worth checking out.</li>
<li>It is beneficial to read the summary of compiled model and calculate the number of parameters again by hand. This could help you comprehend the setup better.</li>
</ul>
<p>Hopefully this post can be helpful to you. Thank you for reading.</p>

        </div>
        <!-- Share to Twitter, Weibo, Telegram -->
        <div class="flex items-center">
          <div class="mr-4 flex items-center">
            <i class="ri-share-forward-line text-gray-500"></i>
          </div>
          <div class="px-4 cursor-pointer text-blue-500 hover:bg-blue-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTwitter">
            <i class="ri-twitter-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-red-500 hover:bg-red-100 dark:hover:bg-gray-600 inline-flex" @click="shareToWeibo">
            <i class="ri-weibo-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-indigo-500 hover:bg-indigo-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTelegram">
            <i class="ri-telegram-line"></i>
          </div>
        </div>
      </div>

      

      

      <footer class="py-12 text-center px-4 md:px-0" v-pre>
  Prudence is a fountain of life to the prudent
</footer>
    </div>

    <!-- TOC Container -->
    <div class="fixed right-0 bottom-0 mb-16 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white dark:bg-gray-500 dark:text-gray-200 hover:shadow-lg transition-all animated fadeInRight" @click="showToc = true">
      <i class="ri-file-list-line"></i>
    </div>

    <div class="fixed right-0 top-0 bottom-0 overflow-y-auto w-64 bg-white dark:bg-gray-800 p-4 border-l border-gray-100 dark:border-gray-600 z-10 transition-fast" :class="{ '-mr-64': !showToc }">
      <div class="flex mb-4 justify-end">
        <div class="w-8 h-8 inline-flex justify-center items-center rounded-full cursor-pointer hover:bg-gray-200 dark:hover:bg-gray-600 transition-fast" @click="showToc = false">
          <i class="ri-close-line text-lg"></i>
        </div>
      </div>
      <div class="post-toc-container">
        <ul class="markdownIt-TOC">
<li><a href="#neural-network-basics">Neural Network Basics</a></li>
<li><a href="#feed-forward-neural-network-using-keras-and-tensorflow">Feed-Forward Neural Network using Keras and TensorFlow</a>
<ul>
<li><a href="#1-have-a-grasp-of-mnist">1. Have a grasp of MNIST</a></li>
<li><a href="#2-implementation">2. Implementation</a></li>
<li><a href="#3-parameter-tuning">3. Parameter Tuning</a>
<ul>
<li><a href="#a-increase-the-number-of-hidden-units-to-128">(a) Increase the number of hidden units to 128</a></li>
<li><a href="#b-change-the-activation-function-to-relu">(b) Change the activation function to reLU</a></li>
<li><a href="#c-change-the-optimizer-to-rmsprop">(c) Change the optimizer to RMSprop</a></li>
<li><a href="#d-try-to-run-the-net-for-ten-epochs-and-use-early-stopping-for-regularization">(d) Try to run the net for ten epochs and use early stopping for regularization</a></li>
<li><a href="#e-add-a-second-layer-with-128-hidden-units">(e) Add a second layer with 128 hidden units.</a></li>
<li><a href="#f-add-dropout">(f) Add dropout.</a></li>
</ul>
</li>
<li><a href="#3-try-to-improve-the-network-architecture">3. Try to improve the network architecture</a></li>
<li><a href="#compute-the-accuracy-precision-and-recall-of-the-designed-model">Compute the accuracy, precision, and recall of the designed model</a></li>
</ul>
</li>
<li><a href="#ending">Ending</a>
<ul>
<li><a href="#challenges">Challenges</a></li>
<li><a href="#tips">Tips</a></li>
</ul>
</li>
</ul>

      </div>
    </div>

    <!-- Back to top -->
    <div class="fixed right-0 bottom-0 mb-4 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white hover:shadow-lg transition-all dark:bg-gray-500 dark:text-gray-200" @click="backToUp" v-show="scrolled">
      <i class="ri-arrow-up-line"></i>
    </div>
  </div>

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
  <!-- Background of PhotoSwipe. 
        It's a separate element as animating opacity is faster than rgba(). -->
  <div class="pswp__bg">
  </div>
  <!-- Slides wrapper with overflow:hidden. -->
  <div class="pswp__scroll-wrap">
    <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
    <div class="pswp__container">
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
    </div>
    <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
    <div class="pswp__ui pswp__ui--hidden">
      <div class="pswp__top-bar">
        <!--  Controls are self-explanatory. Order can be changed. -->
        <div class="pswp__counter">
        </div>
        <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
        <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
        <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
        <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
        <!-- element will get class pswp__preloader--active when preloader is running -->
        <div class="pswp__preloader">
          <div class="pswp__preloader__icn">
            <div class="pswp__preloader__cut">
              <div class="pswp__preloader__donut">
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
        <div class="pswp__share-tooltip">
        </div>
      </div>
      <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
      </button>
      <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
      </button>
      <div class="pswp__caption">
        <div class="pswp__caption__center">
        </div>
      </div>
    </div>
  </div>
</div>

  <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
  <script src="https://BolinWu-Gridea.github.io/media/scripts/main.js"></script>
  
  <!-- Code Highlight -->
  
    <script src="https://BolinWu-Gridea.github.io/media/prism.js"></script>
    <script>
      Prism.highlightAll()
    </script>
  

  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>
  <script>
    //拿到预览框架，也就是上面的html代码
    var pswpElement = document.querySelectorAll('.pswp')[0];
    //定义图片数组变量
    var imgitems;
    /**
    * 用于显示预览界面
    * @param index 图片数组下标
    */
    function viewImg(index) {
      //其它选项这里不做过多阐述，详情见官网
      var pswpoptions = {
        index: parseInt(index, 10), // 开始幻灯片索引。0是第一张幻灯片。必须是整数，而不是字符串。
        bgOpacity: 0.7, // 背景透明度，0-1
        maxSpreadZoom: 3, // 缩放级别，不要太大
      };
      //初始化并打开PhotoSwipe，pswpElement对应上面预览框架，PhotoSwipeUI_Default为皮肤，imgitems为图片数组，pswpoptions为选项
      var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, imgitems, pswpoptions);
      gallery.init()
    }
    /**
    * 用于添加图片点击事件
    * @param img 图片元素
    * @param index 所属下标（在imgitems中的位置）
    */
    function addImgClick(img, index) {
      img.onclick = function() {
        viewImg(index)
      }
    }
    /**
    * 轮询所有图片，获取src、width、height等数据，加入imgitems，并给图片元素添加事件
    * 最好在onload中执行该方法，本站因放在最底部，所以直接初始化
    * 异步加载图片可在图片元素创建完成后调用此方法
    */
    function initImg() {
      //重置图片数组
      imgitems = [];
      //查找class:markdown 下的所有img元素并遍历
      var imgs = document.querySelectorAll('.markdown img');
      for (var i = 0; i < imgs.length; i++) {
        var img = imgs[i];
        //本站相册初始为loading图片，真实图片放在data-src
        var ds = img.getAttribute("data-src");
        //创建image对象，用于获取图片宽高
        var imgtemp = new Image();
        //判断是否存在data-src
        if (ds != null && ds.length > 0) {
          imgtemp.src = ds
        } else {
          imgtemp.src = img.src
        }
        //判断是否存在缓存
        if (imgtemp.complete) {
          var imgobj = {
            "src": imgtemp.src,
            "w": imgtemp.width,
            "h": imgtemp.height,
          };
          imgitems[i] = imgobj;
          addImgClick(img, i);
        } else {
          console.log('进来了2')
          imgtemp.index = i;
          imgtemp.img = img;
          imgtemp.onload = function() {
            var imgobj = {
              "src": this.src,
              "w": this.width,
              "h": this.height,
            };
            //不要使用push，因为onload前后顺序会不同
            imgitems[this.index] = imgobj
            //添加点击事件
            addImgClick(this.img, this.index);
          }
        }
      }
    }
    //初始化
    initImg();
  </script>
  
  
</body>

</html>