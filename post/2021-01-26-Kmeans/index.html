<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>K-means Clustering Algorithm from Scratch  - Bolin Wu</title>
<link rel="shortcut icon" href="https://BolinWu-Gridea.github.io/favicon.ico">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css">
<link rel="stylesheet" href="https://BolinWu-Gridea.github.io/media/css/tailwind.css">
<link rel="stylesheet" href="https://BolinWu-Gridea.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="K-means Clustering Algorithm from Scratch  - Bolin Wu - Atom Feed" href="https://BolinWu-Gridea.github.io/atom.xml">

    

  <meta name="description" content="
The k-means algorithm is a well-known unsupervised machine learning algorithm. From The elements of Statistical Learnin..." />
  <meta property="og:title" content="K-means Clustering Algorithm from Scratch  - Bolin Wu">
  <meta property="og:description" content="
The k-means algorithm is a well-known unsupervised machine learning algorithm. From The elements of Statistical Learnin..." />
  <meta property="og:type" content="articles">
  <meta property="og:url" content="https://BolinWu-Gridea.github.io/post/2021-01-26-Kmeans/" />
  <meta property="og:image" content="https://BolinWu-Gridea.github.io/images/avatar.png">
  <meta property="og:image:height" content="630">
  <meta property="og:image:width" content="1200">
  <meta name="twitter:title" content="K-means Clustering Algorithm from Scratch  - Bolin Wu">
  <meta name="twitter:description" content="
The k-means algorithm is a well-known unsupervised machine learning algorithm. From The elements of Statistical Learnin...">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="canonical" href="https://BolinWu-Gridea.github.io/post/2021-01-26-Kmeans/">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
 
  
    <link rel="stylesheet" href="https://BolinWu-Gridea.github.io/media/css/prism-atom-dark.css">
  

  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
  
</head>

<body>
  <div class="antialiased flex flex-col min-h-screen" id="app">
    <a href="https://BolinWu-Gridea.github.io" class="fixed top-0 left-0 mt-4 bg-black text-white dark:text-gray-700 dark:bg-yellow-50 dark:hover:bg-black dark:hover:text-white inline-flex p-2 pl-8 hover:text-gray-700 hover:bg-yellow-50 font-bold z-10 transition-fast animated fadeInLeft">
      Bolin Wu
    </a>
    <div class="max-w-4xl w-full mx-auto">
      <div class="shadow-box bg-white dark:bg-gray-600 rounded-lg pt-32 md:pt-64 px-4 md:px-8 pb-8 animated fadeIn mb-8">
        <h1 class="text-5xl font-semibold leading-normal pb-8 mb-8 border-b-8 border-gray-700">
          K-means Clustering Algorithm from Scratch 
        </h1>
        
        <div class="mb-8 flex flex-wrap">
          <div class="text-gray-400 text-sm mr-4">2021-01-23 · 10 min read</div>
          
            <a href="https://BolinWu-Gridea.github.io/tag/dNli40GT9/" class="text-gray-700 text-sm border-b-2 border-dotted border-gray-200 hover:border-gray-600 transition-all duration-100 inline-flex mr-2">
              <i class="ri-hashtag"></i>
              R
            </a>
          
            <a href="https://BolinWu-Gridea.github.io/tag/J1GvUL627h/" class="text-gray-700 text-sm border-b-2 border-dotted border-gray-200 hover:border-gray-600 transition-all duration-100 inline-flex mr-2">
              <i class="ri-hashtag"></i>
              unsupervised machine learning
            </a>
          
        </div>
        <div class="markdown mb-8" v-pre>
          <!-- more -->
<p>The k-means algorithm is a well-known unsupervised machine learning algorithm. From <em>The elements of Statistical Learning</em> by Trevor Hastie, Robert Tibshirani and Jerome Friedman, the biggest <strong>difference</strong> between supervised learning and unsupervised learning is that the data for supervised learning are <strong>labeled</strong> while the data for unsupervised learning are <strong>unlabeled</strong>. Statistically speaking, for unsupervised learning, we would like to explore the property of joint density P(X) where X is a p-vector. <br>
In genral, Unsupervised learning is a useful tool to explore the structure of the data and data clustering.</p>
<p>In this blog, I will show the intuition, implementation and evaluation of k-means algorithm.</p>
<p>Reference of this blog:</p>
<ul>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning by Hastie et al. (2009)</a> Chapter 14.</li>
</ul>
<p>Prerequisite to read the following blog:</p>
<ul>
<li>Recommend reading the reference above if you have not touched upon k-means algorithm before.</li>
<li>Basic knowledge of R programming (function and loop).</li>
</ul>
<h1 id="the-k-means-algorithm">The k-means Algorithm</h1>
<p>Basically, K-means clustering starts with a guess for the K cluster centres and then it loops over the following steps until convergence:</p>
<ol>
<li>Assign each data point to the closest cluster centre. Usually the distance is identified as Euclidean distance.</li>
<li>Update the cluster centre by the coordinate-wise average of all data points that are closest to it.</li>
</ol>
<h2 id="data">Data</h2>
<p>To facilitate replication, we will use classic &quot;iris&quot; dataset. We will only use the two variables: <em>Petal.length</em> and <em>Petal.width</em> in order to have a two-dimension visualization.<br>
Load the data and visualize the iris as a scatterplot based on the two variables:</p>
<pre><code class="language-R">library(ggplot2)
data(&quot;iris&quot;)
ggplot(data=iris,aes(x=Petal.Width,y=Petal.Length,color = Species)) +
    geom_point() +
    theme_minimal()
</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621682408696.png" alt="" loading="lazy"><br>
<em>Fig.1 Data visualization</em></p>
<h2 id="implementation">Implementation</h2>
<p>Now we are going to implement different parts of the k-means algorithm by following Algorithm 14.1 in <em>Hastie et al. (2009)</em>.<br>
First step: We will implement a function called <strong>compute_cluster_means(X,C)</strong>. X is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n \times p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span> original data matrix, where n is the number of obsevations, p is the  number of variables. C is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> vector of cluster assignments. The ouput matrix is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">K \times p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span> matrix where K is the total number of clusters and each row represents the cluster centres. <br>
We could assume the number of cluster to be K and run a loop over the K clusters. However, to make understanding easier, let us suppose that the number of clusters is 3:</p>
<pre><code class="language-R">
compute_cluster_means = function(X,C){
  X = cbind(X,C)
  # transforma to data frame so we can use filter funciton
  # in tidyverse
  K1 = colMeans(as.data.frame(X) %&gt;% filter(C==1))
  K2 = colMeans(as.data.frame(X) %&gt;% filter(C==2))
  K3 = colMeans(as.data.frame(X) %&gt;% filter(C==3))
  return(rbind(K1,K2,K3))
}

# set up the X and C
X = as.matrix(iris[,3:4])
# for the first step, assign random clusters, store in C
C = sample(1:3, nrow(X), replace = TRUE)

m = compute_cluster_means(X, C)
m

   Petal.Length Petal.Width C
K1     3.744444    1.244444 1
K2     3.573469    1.095918 2
K3     3.965957    1.255319 3
</code></pre>
<p>Second step: Assign each observation in the original data to each cluster based on the Euclidean distance. <br>
A function called <strong>compute_cluster_encoding(X,m)</strong> will be implemented. m is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">K \times p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span> which calculated above. A <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> vector of cluster assignments will be returned.</p>
<pre><code class="language-R">
compute_coluster_encoding = function(X,m){
 K = 3 # number of clusters
  C = c(NA) # store the encoding
  for (i in 1:nrow(X)) {
    c_dist = c() # store the distances
  for (c in 1:K) { # calculate the distance for each K
    c = sum((X[i,] - m[c,1:2])^2) # defination of dissimilarity for K-means
    c_dist = append(c_dist,c)

  }
    # return the position of the smallest value
    C[i] = which.min(c_dist)
  }
    return(C)
}

C = compute_coluster_encoding(X,m)
# check the first 10 observations' clusters
C[1:10]

[1] 2 2 2 2 2 2 2 2 2 2
</code></pre>
<p>Third step: Reiterate the process by using the two functions defined above until no cluster assignments change. <br>
<em>Note: The initial cluster assigment is randomized</em></p>
<pre><code class="language-R">
update_cluster = function(C_update, X){
  C = rep(0,length = length(C_update))
  # run until the vector C does not change
  while (any(C != C_update)) {
  # let C = initial value for the first loop
  C = C_update
  # m for the first loop is a random initialization value
  m &lt;- compute_cluster_means(X, C)
  # update C
  C_update = compute_coluster_encoding(X,m)
  }
  return(C)
}


set.seed(4711)
C_update &lt;- sample(1:3, nrow(X), replace = TRUE)
head(n = 60, update_cluster(C_update,X))

[1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[39] 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
</code></pre>
<h2 id="evaluation">Evaluation</h2>
<p>Now we have finished the main parts of k-means algorithm. <br>
However, as you may notice, since the initial cluster assignment is randomized therefore theoretically, we will get different final clustering results by using different initial clusters. To evaluate the performance of the algorithm, we will use <strong>k-means within point scatter</strong> as defined in Eq. 14.31 in <em>Hastie et al. (2009)</em>.</p>
<p>The basic idea of calculating k-means within-point scatter is to first find the mean vector of each cluster, and sum up the sqaured euclidean distance of the observations within each cluster. <br>
This is the critera to choose the best K-means clustering.</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi><mo>(</mo><mi>C</mi><mo>)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>N</mi><mi>k</mi></msub><munder><mo>∑</mo><mrow><mi>C</mi><mo>(</mo><mi>i</mi><mo>)</mo><mo>=</mo><mi>k</mi></mrow></munder><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><msub><mi>x</mi><mi>k</mi></msub><mo>ˉ</mo></mover><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">    W(C) = \sum_{k=1}^{K} N_{k} \sum_{C(i) = k} (x_{i} - \bar{x_{k}})^{2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.344341em;vertical-align:-1.516005em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.302113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.808995em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span><span class="mrel mtight">=</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516005em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">ˉ</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><msub><mi>x</mi><mi>k</mi></msub><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{x_{k}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.71778em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">ˉ</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span> is the mean vector associated with the kth cluster and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">N_{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the number of observations in cluster K.</p>
<pre><code class="language-R">k_means_W = function(X,C){
  m = compute_cluster_means(X,C)
  # split the original dataset into different clusters based on the initial random clusters
  X = cbind(X,C)
  G1 = as.data.frame(X) %&gt;% filter(C==1)
  G2 = as.data.frame(X) %&gt;% filter(C==2)
  G3 = as.data.frame(X) %&gt;% filter(C==3)

  # calculate the sqaured euclidean distance within each cluster
  # sweep is used here to substract each row of G1 from m[i,1:2], where i = 1,2,3
  dist_g1 = (sweep(G1, 2, m[1,])[,-3])^2
  dist_g2 = (sweep(G2, 2, m[2,])[,-3])^2
  dist_g3 = (sweep(G3, 2, m[3,])[,-3])^2

  # sum up the sqaured euclidean distance
  return(sum(dist_g1,dist_g2,dist_g3))

}
# get the within pint scatter
k_means_W(X,C)

88.72578
</code></pre>
<p>Next, we can run the k-means a couple times and see which gives the smallest within pint scatter:</p>
<pre><code class="language-R"># number of clusters
K = 3
# the third and fourth columns are Petal.Length and Petal.Width
X = as.matrix(iris[,3:4])
loop = 10
# store the within-point scatter
W_loop_iris = rep(0,length = loop)
# store the final cluster result
C_loop_iris = matrix(NA,nrow = nrow(X),ncol = loop)
for (i in 1:loop) {
  C_update &lt;- sample(1:K, nrow(X), replace = TRUE)
  C_final = update_cluster(C_update,X)
  # k_means_W(X,C_final)
  W_loop_iris[i] = k_means_W(X,C_final)
  # compute_cluster_means(X,C_final)
  C_loop_iris[,i] =C_final
}

W_loop_iris
head(C_loop_iris)

[1] 31.37136 31.37136 31.37136 31.37136 31.37136 31.37136 31.37136 31.37136
 [9] 31.37136 31.37136
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    2    1    1    1    1    3    3    2     2
[2,]    1    2    1    1    1    1    3    3    2     2
[3,]    1    2    1    1    1    1    3    3    2     2
[4,]    1    2    1    1    1    1    3    3    2     2
[5,]    1    2    1    1    1    1    3    3    2     2
[6,]    1    2    1    1    1    1    3    3    2     2
</code></pre>
<p>The assignments for different loops are different but the within point scatters are the same.<br>
I think the main reason is the iris dataset has a clear structure by itself, as we can see in the Figure 1, so that the k-means algorithm just ends up with the same within point scatter every time for different initial clusters. If the structure is not that clear, then we may end up with different clusters and thus have different within point scatters. Another reason is that our number of clusters is only 3, if we increase the number of clusters the final within point scatters may change as well.</p>
<p>Finally, let us visualize the final cluster result:</p>
<pre><code class="language-R"># get the final clustering (first column)
C_final_iris = C_loop_iris[, 1]
iris_visual = cbind(iris[,3:4],C_final_iris)
iris_visual[,&quot;C_final_iris&quot;] = factor(iris_visual[,&quot;C_final_iris&quot;])

ggplot(data=iris_visual,aes(x=Petal.Width, y=Petal.Length,color = C_final_iris))
  + geom_point()
  + theme_minimal()
  + ggtitle(&quot;First Final K-means Clustering Result of iris&quot;) +
    theme(plot.title = element_text(hjust = 0.5))

</code></pre>
<p><img src="https://BolinWu-Gridea.github.io/post-images/1621682471390.png" alt="" loading="lazy"><br>
<em>Fig.2 K-means Clustering Result</em></p>
<p>It is pretty similar to the Fig.1 which indicates that the clustering is reasonable.</p>
<h1 id="potential-improvements">Potential improvements</h1>
<ul>
<li>Use a dataset that does not have a clear structure so that we can have more variations of the clustering results.</li>
<li>When writing the algorithm, we can set the K as a parameter and run the loop instead of repeating. Try different number of K and see how the results differ.</li>
</ul>
<p>Thank you for reading!</p>

        </div>
        <!-- Share to Twitter, Weibo, Telegram -->
        <div class="flex items-center">
          <div class="mr-4 flex items-center">
            <i class="ri-share-forward-line text-gray-500"></i>
          </div>
          <div class="px-4 cursor-pointer text-blue-500 hover:bg-blue-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTwitter">
            <i class="ri-twitter-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-red-500 hover:bg-red-100 dark:hover:bg-gray-600 inline-flex" @click="shareToWeibo">
            <i class="ri-weibo-line"></i>
          </div>
          <div class="px-4 cursor-pointer text-indigo-500 hover:bg-indigo-100 dark:hover:bg-gray-600 inline-flex" @click="shareToTelegram">
            <i class="ri-telegram-line"></i>
          </div>
        </div>
      </div>

      

      

      <footer class="py-12 text-center px-4 md:px-0" v-pre>
  Prudence is a fountain of life to the prudent.
</footer>
    </div>

    <!-- TOC Container -->
    <div class="fixed right-0 bottom-0 mb-16 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white dark:bg-gray-500 dark:text-gray-200 hover:shadow-lg transition-all animated fadeInRight" @click="showToc = true">
      <i class="ri-file-list-line"></i>
    </div>

    <div class="fixed right-0 top-0 bottom-0 overflow-y-auto w-64 bg-white dark:bg-gray-800 p-4 border-l border-gray-100 dark:border-gray-600 z-10 transition-fast" :class="{ '-mr-64': !showToc }">
      <div class="flex mb-4 justify-end">
        <div class="w-8 h-8 inline-flex justify-center items-center rounded-full cursor-pointer hover:bg-gray-200 dark:hover:bg-gray-600 transition-fast" @click="showToc = false">
          <i class="ri-close-line text-lg"></i>
        </div>
      </div>
      <div class="post-toc-container">
        <ul class="markdownIt-TOC">
<li><a href="#the-k-means-algorithm">The k-means Algorithm</a>
<ul>
<li><a href="#data">Data</a></li>
<li><a href="#implementation">Implementation</a></li>
<li><a href="#evaluation">Evaluation</a></li>
</ul>
</li>
<li><a href="#potential-improvements">Potential improvements</a></li>
</ul>

      </div>
    </div>

    <!-- Back to top -->
    <div class="fixed right-0 bottom-0 mb-4 mr-4 shadow w-8 h-8 rounded-full flex justify-center items-center z-10 cursor-pointer bg-white hover:shadow-lg transition-all dark:bg-gray-500 dark:text-gray-200" @click="backToUp" v-show="scrolled">
      <i class="ri-arrow-up-line"></i>
    </div>
  </div>

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
  <!-- Background of PhotoSwipe. 
        It's a separate element as animating opacity is faster than rgba(). -->
  <div class="pswp__bg">
  </div>
  <!-- Slides wrapper with overflow:hidden. -->
  <div class="pswp__scroll-wrap">
    <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
    <div class="pswp__container">
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
      <div class="pswp__item">
      </div>
    </div>
    <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
    <div class="pswp__ui pswp__ui--hidden">
      <div class="pswp__top-bar">
        <!--  Controls are self-explanatory. Order can be changed. -->
        <div class="pswp__counter">
        </div>
        <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
        <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
        <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
        <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
        <!-- element will get class pswp__preloader--active when preloader is running -->
        <div class="pswp__preloader">
          <div class="pswp__preloader__icn">
            <div class="pswp__preloader__cut">
              <div class="pswp__preloader__donut">
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
        <div class="pswp__share-tooltip">
        </div>
      </div>
      <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
      </button>
      <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
      </button>
      <div class="pswp__caption">
        <div class="pswp__caption__center">
        </div>
      </div>
    </div>
  </div>
</div>

  <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
  <script src="https://BolinWu-Gridea.github.io/media/scripts/main.js"></script>
  
  <!-- Code Highlight -->
  
    <script src="https://BolinWu-Gridea.github.io/media/prism.js"></script>
    <script>
      Prism.highlightAll()
    </script>
  

  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>
  <script>
    //拿到预览框架，也就是上面的html代码
    var pswpElement = document.querySelectorAll('.pswp')[0];
    //定义图片数组变量
    var imgitems;
    /**
    * 用于显示预览界面
    * @param index 图片数组下标
    */
    function viewImg(index) {
      //其它选项这里不做过多阐述，详情见官网
      var pswpoptions = {
        index: parseInt(index, 10), // 开始幻灯片索引。0是第一张幻灯片。必须是整数，而不是字符串。
        bgOpacity: 0.7, // 背景透明度，0-1
        maxSpreadZoom: 3, // 缩放级别，不要太大
      };
      //初始化并打开PhotoSwipe，pswpElement对应上面预览框架，PhotoSwipeUI_Default为皮肤，imgitems为图片数组，pswpoptions为选项
      var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, imgitems, pswpoptions);
      gallery.init()
    }
    /**
    * 用于添加图片点击事件
    * @param img 图片元素
    * @param index 所属下标（在imgitems中的位置）
    */
    function addImgClick(img, index) {
      img.onclick = function() {
        viewImg(index)
      }
    }
    /**
    * 轮询所有图片，获取src、width、height等数据，加入imgitems，并给图片元素添加事件
    * 最好在onload中执行该方法，本站因放在最底部，所以直接初始化
    * 异步加载图片可在图片元素创建完成后调用此方法
    */
    function initImg() {
      //重置图片数组
      imgitems = [];
      //查找class:markdown 下的所有img元素并遍历
      var imgs = document.querySelectorAll('.markdown img');
      for (var i = 0; i < imgs.length; i++) {
        var img = imgs[i];
        //本站相册初始为loading图片，真实图片放在data-src
        var ds = img.getAttribute("data-src");
        //创建image对象，用于获取图片宽高
        var imgtemp = new Image();
        //判断是否存在data-src
        if (ds != null && ds.length > 0) {
          imgtemp.src = ds
        } else {
          imgtemp.src = img.src
        }
        //判断是否存在缓存
        if (imgtemp.complete) {
          var imgobj = {
            "src": imgtemp.src,
            "w": imgtemp.width,
            "h": imgtemp.height,
          };
          imgitems[i] = imgobj;
          addImgClick(img, i);
        } else {
          console.log('进来了2')
          imgtemp.index = i;
          imgtemp.img = img;
          imgtemp.onload = function() {
            var imgobj = {
              "src": this.src,
              "w": this.width,
              "h": this.height,
            };
            //不要使用push，因为onload前后顺序会不同
            imgitems[this.index] = imgobj
            //添加点击事件
            addImgClick(this.img, this.index);
          }
        }
      }
    }
    //初始化
    initImg();
  </script>
  
  
</body>

</html>